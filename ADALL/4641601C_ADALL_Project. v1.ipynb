{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8N2s/JjnMph3p+QfBFISA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alpie13-blip/ADALL_github/blob/main/ADALL/4641601C_ADALL_Project.%20v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bc25f18d"
      },
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "# Modelling and preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "33b8ce19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "0716df86-e604-42d7-b214-8d2923ffaa18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded data from GitHub!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    type  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
              "0  white            7.0              0.27         0.36            20.7   \n",
              "1  white            6.3              0.30         0.34             1.6   \n",
              "2  white            8.1              0.28         0.40             6.9   \n",
              "3  white            7.2              0.23         0.32             8.5   \n",
              "4  white            7.2              0.23         0.32             8.5   \n",
              "\n",
              "   chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
              "0      0.045                 45.0                 170.0   1.0010  3.00   \n",
              "1      0.049                 14.0                 132.0   0.9940  3.30   \n",
              "2      0.050                 30.0                  97.0   0.9951  3.26   \n",
              "3      0.058                 47.0                 186.0   0.9956  3.19   \n",
              "4      0.058                 47.0                 186.0   0.9956  3.19   \n",
              "\n",
              "   sulphates  alcohol  quality  \n",
              "0       0.45      8.8        6  \n",
              "1       0.49      9.5        6  \n",
              "2       0.44     10.1        6  \n",
              "3       0.40      9.9        6  \n",
              "4       0.40      9.9        6  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1a9669a4-5edb-4563-9ade-969895486d3b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>white</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.7</td>\n",
              "      <td>0.045</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>1.0010</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>white</td>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.049</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.9940</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>white</td>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.050</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.9951</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>white</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>white</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a9669a4-5edb-4563-9ade-969895486d3b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1a9669a4-5edb-4563-9ade-969895486d3b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1a9669a4-5edb-4563-9ade-969895486d3b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"Please ensure the URL is correct and the file format is compatible with `pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"white\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fixed acidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6426507605223851,\n        \"min\": 6.3,\n        \"max\": 8.1,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          6.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"volatile acidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.031144823004794868,\n        \"min\": 0.23,\n        \"max\": 0.3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"citric acid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.033466401061363026,\n        \"min\": 0.32,\n        \"max\": 0.4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"residual sugar\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.004855458894208,\n        \"min\": 1.6,\n        \"max\": 20.7,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chlorides\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005787918451395114,\n        \"min\": 0.045,\n        \"max\": 0.058,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.049\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"free sulfur dioxide\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.501724035437993,\n        \"min\": 14.0,\n        \"max\": 47.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          14.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total sulfur dioxide\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 38.848423391432505,\n        \"min\": 97.0,\n        \"max\": 186.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          132.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"density\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00272910241654646,\n        \"min\": 0.994,\n        \"max\": 1.001,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.994\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pH\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11519548602267357,\n        \"min\": 3.0,\n        \"max\": 3.3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sulphates\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03781534080237806,\n        \"min\": 0.4,\n        \"max\": 0.49,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"alcohol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5176871642217911,\n        \"min\": 8.8,\n        \"max\": 10.1,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          9.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quality\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 6,\n        \"max\": 6,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Example: Replace this with the raw URL of your GitHub file\n",
        "github_raw_url = 'https://raw.githubusercontent.com/alpie13-blip/ADALL_github/refs/heads/main/ADALL/wine-quality-white-and-red.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(github_raw_url)\n",
        "    print(\"Successfully loaded data from GitHub!\")\n",
        "    display(df.head())\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    print(\"Please ensure the URL is correct and the file format is compatible with `pd.read_csv`.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "3lnkueU5uyCP",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a9abf80-3382-46b7-d584-9a0cceb8a8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6497 entries, 0 to 6496\n",
            "Data columns (total 13 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   type                  6497 non-null   object \n",
            " 1   fixed acidity         6497 non-null   float64\n",
            " 2   volatile acidity      6497 non-null   float64\n",
            " 3   citric acid           6497 non-null   float64\n",
            " 4   residual sugar        6497 non-null   float64\n",
            " 5   chlorides             6497 non-null   float64\n",
            " 6   free sulfur dioxide   6497 non-null   float64\n",
            " 7   total sulfur dioxide  6497 non-null   float64\n",
            " 8   density               6497 non-null   float64\n",
            " 9   pH                    6497 non-null   float64\n",
            " 10  sulphates             6497 non-null   float64\n",
            " 11  alcohol               6497 non-null   float64\n",
            " 12  quality               6497 non-null   int64  \n",
            "dtypes: float64(11), int64(1), object(1)\n",
            "memory usage: 660.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# Load key from Google Colab Secrets\n",
        "api_key = userdata.get('OPEN_AI_API_KEY')\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=api_key,\n",
        ")"
      ],
      "metadata": {
        "id": "kOIMTgG4lyq5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a preview of ten rows as text first, so that we can use it for sending to LLM API later.\n",
        "data_preview = df.head(10).to_string()\n",
        "print(data_preview)"
      ],
      "metadata": {
        "id": "OcoFdjr6xOBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366f1bd9-5a79-451f-ecfe-9b751d5862f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    type  fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  alcohol  quality\n",
            "0  white            7.0              0.27         0.36            20.7      0.045                 45.0                 170.0   1.0010  3.00       0.45      8.8        6\n",
            "1  white            6.3              0.30         0.34             1.6      0.049                 14.0                 132.0   0.9940  3.30       0.49      9.5        6\n",
            "2  white            8.1              0.28         0.40             6.9      0.050                 30.0                  97.0   0.9951  3.26       0.44     10.1        6\n",
            "3  white            7.2              0.23         0.32             8.5      0.058                 47.0                 186.0   0.9956  3.19       0.40      9.9        6\n",
            "4  white            7.2              0.23         0.32             8.5      0.058                 47.0                 186.0   0.9956  3.19       0.40      9.9        6\n",
            "5  white            8.1              0.28         0.40             6.9      0.050                 30.0                  97.0   0.9951  3.26       0.44     10.1        6\n",
            "6  white            6.2              0.32         0.16             7.0      0.045                 30.0                 136.0   0.9949  3.18       0.47      9.6        6\n",
            "7  white            7.0              0.27         0.36            20.7      0.045                 45.0                 170.0   1.0010  3.00       0.45      8.8        6\n",
            "8  white            6.3              0.30         0.34             1.6      0.049                 14.0                 132.0   0.9940  3.30       0.49      9.5        6\n",
            "9  white            8.1              0.22         0.43             1.5      0.044                 28.0                 129.0   0.9938  3.22       0.45     11.0        6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='type', data=df, palette='viridis')\n",
        "plt.title('Distribution of Wine Types')\n",
        "plt.xlabel('Wine Type')\n",
        "plt.ylabel('Count')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "KCkX7wN_-DYd",
        "outputId": "6a0d1430-f0e3-4e32-e8d2-eb2f171686ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-574174436.py:5: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.countplot(x='type', data=df, palette='viridis')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sending to LLM API\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    instructions=\"\"\"\n",
        "You are an expert data scientist with extensive knowledge of tree-based models.\n",
        "Use ONLY the information inside the dataset profile text.\n",
        "Do NOT invent correlations, columns, or values.\n",
        "If something is not in the dataset profile, state 'Not shown in profile'.\n",
        "Always justify recommendations using reasoning trace based ONLY on the dataset profile.\n",
        "\"\"\",\n",
        "    input=f\"\"\"Dataset info: {data_preview}\\n\n",
        "    Context:\n",
        "    The business problem is that a wine distributor struggles with inconsistent reviews across vintages.\n",
        "    He wants to understand if the chemistry of the wine can classify wines into quality tiers (standard vs premium)\n",
        "    before the reviews are published.\n",
        "\n",
        "    Questions\n",
        "    1. Based on the context and dataset info, how should i approach modelling objective? focus on problem framing aspects.\n",
        "    2. What would be the most meaningful target?\n",
        "    3. What would be most important metric for scoring?\n",
        "    4. What are the top 3 most potentially important features?\n",
        "    \"\"\")\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "id": "00iKrRXmuxzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac298a80-44f2-44b0-b9fd-17dc497b7634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below I frame this as a supervised binary classification task and explain choices using only the columns and values shown in the dataset profile.\n",
            "\n",
            "1) Modelling objective — problem framing\n",
            "- Type of modelling: supervised binary classification (standard vs premium) using the chemical measurements as predictors and the provided quality column as the label source.\n",
            "  - Reasoning trace from profile: the table contains chemical feature columns (fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol, type) and a numeric quality column — this is the natural basis for supervised learning.\n",
            "- Key preparatory steps to include (what to do before training):\n",
            "  - Define the binary target by thresholding the existing quality column (see next section).\n",
            "  - Inspect the full dataset’s quality distribution and class balance (distribution not shown in profile).\n",
            "    - Reasoning trace: quality values are present in the profile but the full distribution / counts are not shown in the sample.\n",
            "  - Split with stratified cross-validation on the binary target to preserve class balance during evaluation.\n",
            "    - Reasoning trace: we will be predicting a binary tier derived from quality; stratification prevents skewed folds.\n",
            "  - Use tree-based learners (e.g., random forest, gradient-boosted trees) as first baselines because they natively handle numeric inputs and require little scaling/standardization.\n",
            "    - Reasoning trace: predictors in the profile are numeric chemical measurements; tree models work directly with such features.\n",
            "  - After initial models, examine feature importances and partial dependence / SHAP to validate which chemistry signals separate tiers (cannot compute actual importances from the shown sample).\n",
            "    - Reasoning trace: importance needs to be measured from the full data; sample rows are insufficient to infer importances.\n",
            "\n",
            "2) Most meaningful target\n",
            "- Construct a binary target: premium vs standard by thresholding the existing quality column.\n",
            "  - Reasoning trace: the profile contains the quality column (all shown rows have quality = 6), so the available target signal is quality.\n",
            "- How to pick the threshold:\n",
            "  - Prefer a business-defined threshold if the distributor has a target definition for “premium” (e.g., reviewer-based threshold). If no business rule exists, define premium as the top X% (percentile) of quality scores or use a threshold such as quality >= T.\n",
            "  - Important caveat: the full quality distribution and counts are Not shown in profile, so the exact threshold should be chosen after inspecting the full dataset (class balance).\n",
            "- Explicit note: the dataset sample only shows quality = 6 for the rows provided; the overall distribution of quality scores across the entire dataset is Not shown in profile.\n",
            "\n",
            "3) Most important metric for scoring\n",
            "- Recommended primary metrics (choose based on business priorities—see justification):\n",
            "  - If the business cares about reliably identifying premium wines (i.e., when the model predicts premium you want high confidence): use precision for the premium class (or precision@k if you will shortlist a fixed number).\n",
            "    - Reasoning trace: the distributor’s stated goal is to identify premium wines before reviews are published — that suggests acting on positive predictions, so precision measures how many predicted premiums are truly premium.\n",
            "  - If both missing premiums and false premium calls matter and costs are symmetric/unclear: use F1-score for the premium class (harmonic mean of precision and recall).\n",
            "  - For overall ranking performance and if you care about score calibration and ranking across all wines: use ROC AUC as a model-agnostic ranking metric.\n",
            "- Practical note: choose the final metric after you inspect class balance (Not shown in profile) and stakeholder cost/preferences (costs of false positives vs false negatives).\n",
            "\n",
            "4) Top 3 most potentially important features (based only on the dataset profile)\n",
            "I cannot compute empirical importances from the sample rows shown, but based on the columns present and the variation visible in the sample rows, the following three are reasonable candidates to test first:\n",
            "- alcohol\n",
            "  - Reasoning trace: alcohol values in the sample vary (e.g., 8.8, 9.5, 10.1, 9.9, 11.0) — a visible spread suggests it may help separate wines.\n",
            "- residual sugar\n",
            "  - Reasoning trace: residual sugar shows a large observed range in the sample rows (1.5, 1.6, 6.9, 7.0, 8.5, 20.7) indicating substantial variation to exploit.\n",
            "- total sulfur dioxide (or free sulfur dioxide as a follow-up)\n",
            "  - Reasoning trace: total sulfur dioxide varies across the shown rows (97.0, 129.0, 132.0, 136.0, 170.0, 186.0) — this spread suggests it’s a potentially informative chemical signal. If you test feature sets, include free sulfur dioxide too (present in the profile) as it may add complementary information.\n",
            "- Additional candidates to consider (run and verify): fixed acidity, pH, volatile acidity, sulphates — these are present in the profile and should be evaluated by the model because the sample shows some variation, but I did not list them in the top three because the sample variation for them is smaller than for the three above.\n",
            "\n",
            "Final caveats and next steps\n",
            "- I cannot provide empirical feature importances, class balances, or an optimal quality threshold because the full dataset statistics are Not shown in the profile; everything above is derived strictly from the column list and the sample rows shown.\n",
            "- Immediate actionable steps:\n",
            "  1. Inspect full quality distribution and decide the premium threshold (business rule or percentile).\n",
            "  2. Train tree-based baselines (random forest / gradient boosting) with stratified CV.\n",
            "  3. Evaluate using precision (premium), F1, and ROC AUC; pick the primary metric based on the stakeholder preference for false positives vs false negatives.\n",
            "  4. Use model explanations (feature importances, SHAP) to confirm which chemistry features actually drive the premium predictions on your full dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(df['quality'], bins=range(min(df['quality']), max(df['quality']) + 2), kde=True, stat='count')\n",
        "plt.title('Distribution of Wine Quality')\n",
        "plt.xlabel('Quality')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(range(min(df['quality']), max(df['quality']) + 1))\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "pz6XL4QgafhJ",
        "outputId": "d5290182-9d35-4634-a2bb-af635f964239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate statistics\n",
        "q1 = df['quality'].quantile(0.25)\n",
        "q3 = df['quality'].quantile(0.75)\n",
        "median = df['quality'].median()\n",
        "iqr = q3 - q1\n",
        "\n",
        "print(f\"Q1 (25th percentile): {q1}\")\n",
        "print(f\"Median (50th percentile): {median}\")\n",
        "print(f\"Q3 (75th percentile): {q3}\")\n",
        "print(f\"IQR (Interquartile Range): {iqr}\")\n",
        "\n",
        "# Generate boxplot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x=df['quality'])\n",
        "plt.title('Boxplot of Wine Quality with Statistics')\n",
        "plt.xlabel('Quality')\n",
        "plt.grid(axis='x', alpha=0.75)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "U3xdcCAWa_d0",
        "outputId": "a9b5e340-7888-4bcb-bd18-f2879f1ce39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1 (25th percentile): 5.0\n",
            "Median (50th percentile): 6.0\n",
            "Q3 (75th percentile): 6.0\n",
            "IQR (Interquartile Range): 1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Now let us prepare a payload of information that the LLM can use for deeper analysis and clearer recommendations."
      ],
      "metadata": {
        "id": "31MNN6m96sC2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d838dc80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dae060d-5595-47cd-821d-cbe05ceed55c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DTYPES ===\n",
            "type                     object\n",
            "fixed acidity           float64\n",
            "volatile acidity        float64\n",
            "citric acid             float64\n",
            "residual sugar          float64\n",
            "chlorides               float64\n",
            "free sulfur dioxide     float64\n",
            "total sulfur dioxide    float64\n",
            "density                 float64\n",
            "pH                      float64\n",
            "sulphates               float64\n",
            "alcohol                 float64\n",
            "quality                   int64\n",
            "\n",
            "=== NUMERIC DESCRIBE ===\n",
            "       fixed acidity  volatile acidity  citric acid  residual sugar    chlorides  free sulfur dioxide  total sulfur dioxide      density           pH    sulphates      alcohol      quality\n",
            "count    6497.000000       6497.000000  6497.000000     6497.000000  6497.000000          6497.000000           6497.000000  6497.000000  6497.000000  6497.000000  6497.000000  6497.000000\n",
            "mean        7.215307          0.339666     0.318633        5.443235     0.056034            30.525319            115.744574     0.994697     3.218501     0.531268    10.491801     5.818378\n",
            "std         1.296434          0.164636     0.145318        4.757804     0.035034            17.749400             56.521855     0.002999     0.160787     0.148806     1.192712     0.873255\n",
            "min         3.800000          0.080000     0.000000        0.600000     0.009000             1.000000              6.000000     0.987110     2.720000     0.220000     8.000000     3.000000\n",
            "25%         6.400000          0.230000     0.250000        1.800000     0.038000            17.000000             77.000000     0.992340     3.110000     0.430000     9.500000     5.000000\n",
            "50%         7.000000          0.290000     0.310000        3.000000     0.047000            29.000000            118.000000     0.994890     3.210000     0.510000    10.300000     6.000000\n",
            "75%         7.700000          0.400000     0.390000        8.100000     0.065000            41.000000            156.000000     0.996990     3.320000     0.600000    11.300000     6.000000\n",
            "max        15.900000          1.580000     1.660000       65.800000     0.611000           289.000000            440.000000     1.038980     4.010000     2.000000    14.900000     9.000000\n",
            "\n",
            "=== CATEGORICAL DESCRIBE ===\n",
            "         type\n",
            "count    6497\n",
            "unique      2\n",
            "top     white\n",
            "freq     4898\n",
            "\n",
            "=== NULL SUMMARY ===\n",
            "                      null_count  null_pct\n",
            "type                           0       0.0\n",
            "fixed acidity                  0       0.0\n",
            "volatile acidity               0       0.0\n",
            "citric acid                    0       0.0\n",
            "residual sugar                 0       0.0\n",
            "chlorides                      0       0.0\n",
            "free sulfur dioxide            0       0.0\n",
            "total sulfur dioxide           0       0.0\n",
            "density                        0       0.0\n",
            "pH                             0       0.0\n",
            "sulphates                      0       0.0\n",
            "alcohol                        0       0.0\n",
            "quality                        0       0.0\n",
            "\n",
            "=== UNIQUE VALUES PER COLUMN ===\n",
            "                      unique_count\n",
            "type                             2\n",
            "fixed acidity                  106\n",
            "volatile acidity               187\n",
            "citric acid                     89\n",
            "residual sugar                 316\n",
            "chlorides                      214\n",
            "free sulfur dioxide            135\n",
            "total sulfur dioxide           276\n",
            "density                        998\n",
            "pH                             108\n",
            "sulphates                      111\n",
            "alcohol                        111\n",
            "quality                          7\n",
            "\n",
            "=== CORRELATIONS (NUMERIC ONLY) ===\n",
            "                      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  density     pH  sulphates  alcohol  quality\n",
            "fixed acidity                 1.000             0.219        0.324          -0.112      0.298               -0.283                -0.329    0.459 -0.253      0.300   -0.095   -0.077\n",
            "volatile acidity              0.219             1.000       -0.378          -0.196      0.377               -0.353                -0.414    0.271  0.261      0.226   -0.038   -0.266\n",
            "citric acid                   0.324            -0.378        1.000           0.142      0.039                0.133                 0.195    0.096 -0.330      0.056   -0.010    0.086\n",
            "residual sugar               -0.112            -0.196        0.142           1.000     -0.129                0.403                 0.495    0.553 -0.267     -0.186   -0.359   -0.037\n",
            "chlorides                     0.298             0.377        0.039          -0.129      1.000               -0.195                -0.280    0.363  0.045      0.396   -0.257   -0.201\n",
            "free sulfur dioxide          -0.283            -0.353        0.133           0.403     -0.195                1.000                 0.721    0.026 -0.146     -0.188   -0.180    0.055\n",
            "total sulfur dioxide         -0.329            -0.414        0.195           0.495     -0.280                0.721                 1.000    0.032 -0.238     -0.276   -0.266   -0.041\n",
            "density                       0.459             0.271        0.096           0.553      0.363                0.026                 0.032    1.000  0.012      0.259   -0.687   -0.306\n",
            "pH                           -0.253             0.261       -0.330          -0.267      0.045               -0.146                -0.238    0.012  1.000      0.192    0.121    0.020\n",
            "sulphates                     0.300             0.226        0.056          -0.186      0.396               -0.188                -0.276    0.259  0.192      1.000   -0.003    0.038\n",
            "alcohol                      -0.095            -0.038       -0.010          -0.359     -0.257               -0.180                -0.266   -0.687  0.121     -0.003    1.000    0.444\n",
            "quality                      -0.077            -0.266        0.086          -0.037     -0.201                0.055                -0.041   -0.306  0.020      0.038    0.444    1.000\n",
            "\n",
            "=== VALUE COUNTS (TOP 20 PER CATEGORICAL COLUMN) ===\n",
            "\n",
            "Column: type\n",
            "type\n",
            "white    4898\n",
            "red      1599\n",
            "\n",
            "=== OUTLIER SUMMARY (IQR METHOD) ===\n",
            "fixed acidity           357\n",
            "volatile acidity        377\n",
            "citric acid             509\n",
            "residual sugar          118\n",
            "chlorides               286\n",
            "free sulfur dioxide      62\n",
            "total sulfur dioxide     10\n",
            "density                   3\n",
            "pH                       73\n",
            "sulphates               191\n",
            "alcohol                   3\n",
            "quality                 228\n",
            "\n",
            "=== POSSIBLE LEAKAGE COLUMNS (UNIQUE FOR EACH ROW) ===\n",
            "[]\n",
            "\n",
            "=== SHAPE / DUPLICATES / CONSTANT COLUMNS ===\n",
            "Rows: 6497, Columns: 13\n",
            "Duplicate rows: 1177\n",
            "Constant columns: []\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "\n",
        "# ---------------------------\n",
        "# Generate a full dataset profile\n",
        "# ---------------------------\n",
        "\n",
        "buffer = StringIO()\n",
        "\n",
        "# dtypes\n",
        "buffer.write(\"=== DTYPES ===\\n\")\n",
        "buffer.write(df.dtypes.to_string())\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# numeric describe\n",
        "buffer.write(\"=== NUMERIC DESCRIBE ===\\n\")\n",
        "buffer.write(df.describe().to_string())\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# categorical describe\n",
        "buffer.write(\"=== CATEGORICAL DESCRIBE ===\\n\")\n",
        "try:\n",
        "    buffer.write(df.describe(include='object').to_string())\n",
        "except:\n",
        "    buffer.write(\"No categorical columns\")\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# null summary\n",
        "buffer.write(\"=== NULL SUMMARY ===\\n\")\n",
        "null_summary = (\n",
        "    df.isna().sum().to_frame(\"null_count\")\n",
        "    .assign(null_pct=lambda x: x[\"null_count\"]/len(df))\n",
        ")\n",
        "buffer.write(null_summary.to_string())\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# unique cardinality\n",
        "buffer.write(\"=== UNIQUE VALUES PER COLUMN ===\\n\")\n",
        "buffer.write(df.nunique().to_frame(\"unique_count\").to_string())\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# correlation matrix\n",
        "buffer.write(\"=== CORRELATIONS (NUMERIC ONLY) ===\\n\")\n",
        "buffer.write(df.corr(numeric_only=True).round(3).to_string())\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# value counts for categoricals\n",
        "buffer.write(\"=== VALUE COUNTS (TOP 20 PER CATEGORICAL COLUMN) ===\\n\")\n",
        "cat_cols = df.select_dtypes(include='object').columns\n",
        "if len(cat_cols) > 0:\n",
        "    for col in cat_cols:\n",
        "        buffer.write(f\"\\nColumn: {col}\\n\")\n",
        "        vc = df[col].value_counts().head(20)\n",
        "        buffer.write(vc.to_string())\n",
        "        buffer.write(\"\\n\")\n",
        "else:\n",
        "    buffer.write(\"No categorical columns\\n\")\n",
        "buffer.write(\"\\n\")\n",
        "\n",
        "# --------- FIXED OUTLIER COMPUTATION (NO BOOLEANS) ---------\n",
        "buffer.write(\"=== OUTLIER SUMMARY (IQR METHOD) ===\\n\")\n",
        "num_cols = df.select_dtypes(include=['number']).columns  # exclude booleans\n",
        "Q1 = df[num_cols].quantile(0.25)\n",
        "Q3 = df[num_cols].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = ((df[num_cols] < (Q1 - 1.5*IQR)) | (df[num_cols] > (Q3 + 1.5*IQR))).sum()\n",
        "buffer.write(outliers.to_string())\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# leakage scan: columns with all unique values\n",
        "buffer.write(\"=== POSSIBLE LEAKAGE COLUMNS (UNIQUE FOR EACH ROW) ===\\n\")\n",
        "leak_cols = df.columns[df.nunique() == len(df)]\n",
        "buffer.write(str(list(leak_cols)))\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# shape, duplicates, constant cols\n",
        "buffer.write(\"=== SHAPE / DUPLICATES / CONSTANT COLUMNS ===\\n\")\n",
        "dup_count = df.duplicated().sum()\n",
        "constant_cols = df.columns[df.nunique() == 1].tolist()\n",
        "buffer.write(f\"Rows: {len(df)}, Columns: {df.shape[1]}\\n\")\n",
        "buffer.write(f\"Duplicate rows: {dup_count}\\n\")\n",
        "buffer.write(f\"Constant columns: {constant_cols}\\n\\n\")\n",
        "\n",
        "# Final text\n",
        "payload_text = buffer.getvalue()\n",
        "\n",
        "print(payload_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original template"
      ],
      "metadata": {
        "id": "pkRj21UKwdf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    instructions=\"\"\"\n",
        "You are an expert data scientist with extensive knowledge of tree-based models.\n",
        "Always justify recommendations using reasoning trace based ONLY on the dataset profile.\n",
        "\"\"\",\n",
        "    input=f\"\"\"\n",
        "Dataset info: {payload_text}\\n\n",
        "Questions:\\n\n",
        "1. Based on the dataset profile, what data quality issues should be resolved before modelling?\n",
        "Provide a priority list and justify each item. \\n\n",
        "2. Which columns appear redundant, correlated, or likely to cause leakage?\n",
        "Explain why each is problematic. \\n\n",
        "Next: Provide a python script to handle the identified issues.\n",
        "Dataset is in github\n",
        "Define one helper function for each issue.\n",
        "Then define a wrapper function that calls these helper with true false option as user choice\n",
        "Provide a single line of code to run the overall wrapper function.\n",
        "Do not encode categorical columns or model first.\n",
        "\"\"\")\n",
        "\n",
        "print(response.output_text)\n"
      ],
      "metadata": {
        "id": "dnnaAhFhl2C1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86cde994-7f42-42ee-acf1-dece123e3e6a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below I first list the data‑quality issues you should resolve before modeling (priority order with justification strictly based on the dataset profile you gave). Then I identify redundant/correlated columns and explain why each is problematic (again citing the profile). Finally I provide a reproducible Python script that implements one helper function per issue and a wrapper that calls them under user control. The script reads the dataset from a Github CSV URL you supply. I do not encode the categorical variable or run any models (as requested).\n",
            "\n",
            "1) Priority list of data‑quality issues to resolve (highest → lowest)\n",
            "- 1. Remove duplicate rows (high priority)\n",
            "  - Reasoning from profile: Duplicate rows: 1177. With 6,497 total rows, ~18% duplicates (1177/6497) meaning the dataset has many repeated records that will bias model training, inflate apparent sample size, and cause overly optimistic validation if duplicates leak across train/test splits.\n",
            "- 2. Handle extreme outliers (high priority)\n",
            "  - Reasoning from profile: The IQR outlier summary reports many columns with large numbers of outliers: citric acid (509), volatile acidity (377), fixed acidity (357), chlorides (286), sulphates (191), residual sugar (118), pH (73), etc. Also residual sugar maximum is 65.8 while median is 3.0 and mean 5.44 (high right skew). Such extreme values are likely measurement errors or rare cases which can adversely affect feature splits or create splits on noise; they should be inspected and either winsorized/capped or removed (or transformed) depending on context.\n",
            "- 3. Remove/reduce redundant highly correlated features (medium priority)\n",
            "  - Reasoning from profile: There are strong pairwise correlations: free sulfur dioxide vs total sulfur dioxide (r = 0.721); density vs alcohol (r = -0.687); residual sugar vs density (r = 0.553); total sulfur dioxide vs residual sugar (r = 0.495). Strong correlation between features can create redundancy (larger trees relying on duplicate information) and reduce interpretability; for tree models multicollinearity is less harmful than for linear models but removing one of a highly correlated pair still simplifies the model and reduces noise. Drop the less informative of each highly correlated pair (use correlation with target quality to decide).\n",
            "- 4. Flag and consider handling imbalance in the 'type' feature (low/medium priority depending on modeling choice)\n",
            "  - Reasoning from profile: type counts: white 4,898 vs red 1,599 (ratio ~3.06:1). If you plan to use the 'type' categorical as a predictor, it's imbalanced and may dominate splits if not handled appropriately or may require careful cross‑validation stratified by type. This is a feature imbalance (not target imbalance) so it’s not strictly a data error but must be acknowledged.\n",
            "- 5. Check for and preserve target distribution / review outlier treatment impact on target (medium priority)\n",
            "  - Reasoning from profile: quality has 7 unique values and 228 outlier flags by IQR method. Because quality is the target, be careful not to remove legitimate rare quality scores indiscriminately. Any outlier filtering should avoid discarding valid target variability.\n",
            "- 6. Missing values: no action required (low priority)\n",
            "  - Reasoning from profile: NULL SUMMARY shows null_count = 0 for all columns. So no imputation needed.\n",
            "- 7. Data types: minor check (low priority)\n",
            "  - Reasoning from profile: type is object, numeric columns are floats/ints as expected. No immediate data type fixes required.\n",
            "\n",
            "2) Columns that appear redundant, highly correlated, or likely to cause issues\n",
            "- total sulfur dioxide and free sulfur dioxide\n",
            "  - Why problematic: correlation = 0.721 (profile). They are chemically related measures (one is a subset of the other) and convey largely overlapping information. Keeping both adds redundancy. When dropping one, choose the one with lower absolute correlation with the target (profile shows free sulfur dioxide corr with quality = 0.055 and total sulfur dioxide corr = -0.041; total has lower abs correlation, so drop total_sulfur_dioxide).\n",
            "- density and alcohol\n",
            "  - Why problematic: correlation = -0.687 (profile). This is close to the typical 0.7 cutoff for strong correlation. Density and alcohol strongly move in opposite directions (alcohol lowers density). For tree models this may not break modeling, but it is redundant information; consider keeping the variable with stronger predictive power (alcohol has corr with quality = 0.444, density has corr = -0.306 → keep alcohol, consider dropping density if you want fewer features).\n",
            "- residual sugar and density / residual sugar and total sulfur dioxide\n",
            "  - Why problematic: residual sugar correlates with density (0.553) and with total sulfur dioxide (0.495). These correlations show redundancy; residual sugar can be partially inferred from density for many samples. Keep the most predictive variable (compare each feature's correlation with quality) or keep both if you want to preserve potentially complementary signals (trees can use them differently).\n",
            "- fixed acidity, volatile acidity, citric acid\n",
            "  - Why problematic: these acid measures inter-correlate (fixed acidity vs citric acid 0.324; volatile acidity vs citric acid -0.378) and several show many outliers (fixed acidity 357, volatile acidity 377, citric acid 509 outliers). They are chemically related and may contain overlapping information. Evaluate feature importance and consider dimensionality reduction or dropping less predictive ones.\n",
            "- quality\n",
            "  - Not redundant — it is the target. Note: 228 outlier flags were detected by the IQR method on the quality column; since this is the target, do not drop or naively transform target outliers — inspect them.\n",
            "\n",
            "General guidance for tree models:\n",
            "- Trees are robust to monotonic scaling and some outliers, but extremely large or erroneous outliers and highly duplicated rows still need attention. Redundant features hamper interpretability and may increase training time but are less damaging to predictive performance than for linear models. Use the correlation-with-target heuristic to choose which correlated feature to keep.\n",
            "\n",
            "Python script\n",
            "- The script below implements the actionable items above. It contains one helper function per issue:\n",
            "  - load_data_from_github(url)\n",
            "  - remove_duplicates(df, drop=True)\n",
            "  - report_and_handle_outliers_iqr(df, columns=None, method='cap'/'remove'/'none', cap_quantile=0.01)\n",
            "  - drop_highly_correlated(df, threshold=0.70, target_col='quality')\n",
            "  - flag_type_imbalance(df, cat_col='type', report_only=True)\n",
            "  - basic_type_checks(df)  # data types / null check no-op if clean\n",
            "- A wrapper preprocess_dataset(...) calls the helpers in reasonable order, with boolean flags to enable/disable each step.\n",
            "- The wrapper returns cleaned_df and a simple log dictionary.\n",
            "\n",
            "Notes:\n",
            "- The outlier handler uses the IQR method (consistent with the profile). By default it will cap outliers at the IQR bounds (winsorize) because for many wine physicochemical measures capping preserves sample size while reducing extreme influence; user can choose remove to drop rows with outliers.\n",
            "- For correlated features the drop decision is automated: for each highly correlated pair (abs(r) >= threshold), the function drops the column of the pair with the lower absolute Pearson correlation with the target (so we keep the more predictive column). This implements the \"drop less predictive redundant feature\" recommendation.\n",
            "- The script does not encode 'type' and does not perform modeling, per your request.\n",
            "\n",
            "Script (copy/paste runnable). Set GITHUB_CSV_URL to your CSV file path.\n",
            "\n",
            "```\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "from typing import Tuple, Dict, List\n",
            "\n",
            "# Helper 1: load data from a GitHub raw CSV url (or any CSV URL / local path)\n",
            "def load_data_from_github(url: str) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Load CSV from a URL or local path into a DataFrame.\n",
            "    \"\"\"\n",
            "    df = pd.read_csv(url)\n",
            "    return df\n",
            "\n",
            "# Helper 2: remove duplicates\n",
            "def remove_duplicates(df: pd.DataFrame, drop: bool = True) -> Tuple[pd.DataFrame, Dict]:\n",
            "    \"\"\"\n",
            "    Remove duplicate rows. Returns (df, log).\n",
            "    \"\"\"\n",
            "    before = len(df)\n",
            "    df_dedup = df.drop_duplicates() if drop else df.copy()\n",
            "    after = len(df_dedup)\n",
            "    log = {'duplicates_before': before, 'duplicates_after': after, 'duplicates_removed': before - after}\n",
            "    return df_dedup, log\n",
            "\n",
            "# Helper 3: report and handle outliers using IQR method\n",
            "def report_and_handle_outliers_iqr(\n",
            "    df: pd.DataFrame,\n",
            "    columns: List[str] = None,\n",
            "    method: str = 'cap',\n",
            "    cap_quantile: float = 0.01\n",
            ") -> Tuple[pd.DataFrame, Dict]:\n",
            "    \"\"\"\n",
            "    Identify outliers per column using IQR method.\n",
            "    method: 'cap' to winsorize at IQR bounds, 'remove' to drop rows with any outlier in columns, 'none' to only report.\n",
            "    columns: list of numeric columns to check; if None, check all numeric except target 'quality' by default.\n",
            "    cap_quantile: optional lower/upper quantile to use in addition to IQR bounds (e.g., 0.01 to cap at 1st/99th percentile if desired).\n",
            "    Returns (df_out, log).\n",
            "    \"\"\"\n",
            "    numeric = df.select_dtypes(include=[np.number]).columns.tolist()\n",
            "    # exclude target 'quality' from automatic removal/capping by default\n",
            "    if columns is None:\n",
            "        # choose all numeric except 'quality'\n",
            "        columns = [c for c in numeric if c != 'quality']\n",
            "    logs = {}\n",
            "    df_work = df.copy()\n",
            "    n = len(df_work)\n",
            "    for col in columns:\n",
            "        col_series = df_work[col]\n",
            "        Q1 = col_series.quantile(0.25)\n",
            "        Q3 = col_series.quantile(0.75)\n",
            "        IQR = Q3 - Q1\n",
            "        lower = Q1 - 1.5 * IQR\n",
            "        upper = Q3 + 1.5 * IQR\n",
            "        # optional: further constrain to empirical quantiles to avoid extremely wide caps from small IQR\n",
            "        lower_q = col_series.quantile(cap_quantile)\n",
            "        upper_q = col_series.quantile(1 - cap_quantile)\n",
            "        lower_final = max(lower, lower_q)\n",
            "        upper_final = min(upper, upper_q)\n",
            "        outlier_mask = (col_series < lower_final) | (col_series > upper_final)\n",
            "        count_outliers = int(outlier_mask.sum())\n",
            "        logs[col] = {\n",
            "            'Q1': float(Q1),\n",
            "            'Q3': float(Q3),\n",
            "            'IQR': float(IQR),\n",
            "            'lower_bound': float(lower_final),\n",
            "            'upper_bound': float(upper_final),\n",
            "            'outlier_count': count_outliers,\n",
            "            'outlier_frac': float(count_outliers / n)\n",
            "        }\n",
            "        if method == 'cap':\n",
            "            # cap values to bounds\n",
            "            if count_outliers > 0:\n",
            "                df_work.loc[col_series < lower_final, col] = lower_final\n",
            "                df_work.loc[col_series > upper_final, col] = upper_final\n",
            "        elif method == 'remove':\n",
            "            # drop rows with outliers in this column\n",
            "            if count_outliers > 0:\n",
            "                df_work = df_work.loc[~outlier_mask].reset_index(drop=True)\n",
            "                # After dropping, recompute n for subsequent columns and logs may not reflect final counts exactly\n",
            "        elif method == 'none':\n",
            "            pass\n",
            "        else:\n",
            "            raise ValueError(\"method must be one of {'cap','remove','none'}\")\n",
            "    final_n = len(df_work)\n",
            "    logs['n_before'] = int(n)\n",
            "    logs['n_after'] = int(final_n)\n",
            "    logs['method'] = method\n",
            "    return df_work, logs\n",
            "\n",
            "# Helper 4: drop highly correlated features (numeric) using threshold and target correlation to decide which to drop\n",
            "def drop_highly_correlated(\n",
            "    df: pd.DataFrame,\n",
            "    threshold: float = 0.70,\n",
            "    target_col: str = 'quality',\n",
            "    verbose: bool = False\n",
            ") -> Tuple[pd.DataFrame, Dict]:\n",
            "    \"\"\"\n",
            "    Identify pairs with abs(corr) >= threshold (numeric only). For each pair, drop the column that has lower\n",
            "    absolute Pearson correlation with the target (keep the more predictive feature).\n",
            "    Returns (df_out, log).\n",
            "    \"\"\"\n",
            "    numeric = df.select_dtypes(include=[np.number]).columns.tolist()\n",
            "    # ensure target in numeric list\n",
            "    if target_col not in numeric:\n",
            "        raise ValueError(f\"target_col {target_col} not numeric or not present in df\")\n",
            "    numeric_no_target = [c for c in numeric if c != target_col]\n",
            "    corr = df[numeric].corr().abs()\n",
            "    to_drop = set()\n",
            "    pairs_considered = []\n",
            "    for i, col1 in enumerate(numeric_no_target):\n",
            "        for col2 in numeric_no_target[i+1:]:\n",
            "            r = corr.loc[col1, col2]\n",
            "            if r >= threshold:\n",
            "                # compare absolute corr with target\n",
            "                corr1 = abs(df[[col1, target_col]].corr().iloc[0,1])\n",
            "                corr2 = abs(df[[col2, target_col]].corr().iloc[0,1])\n",
            "                # drop the one with LOWER correlation with the target\n",
            "                if corr1 >= corr2:\n",
            "                    drop_col = col2\n",
            "                    keep_col = col1\n",
            "                else:\n",
            "                    drop_col = col1\n",
            "                    keep_col = col2\n",
            "                if drop_col not in to_drop:\n",
            "                    to_drop.add(drop_col)\n",
            "                    pairs_considered.append({'col_pair': (col1, col2), 'corr': float(r),\n",
            "                                             'drop': drop_col, 'keep': keep_col,\n",
            "                                             'corr_with_target': {col1: float(corr1), col2: float(corr2)}})\n",
            "    df_reduced = df.drop(columns=list(to_drop)) if to_drop else df.copy()\n",
            "    log = {'dropped_columns': list(to_drop), 'pairs_considered': pairs_considered, 'threshold': threshold}\n",
            "    if verbose:\n",
            "        print(\"Dropped columns due to high correlation:\", log['dropped_columns'])\n",
            "    return df_reduced, log\n",
            "\n",
            "# Helper 5: flag type imbalance (no encoding)\n",
            "def flag_type_imbalance(df: pd.DataFrame, cat_col: str = 'type') -> Dict:\n",
            "    \"\"\"\n",
            "    Report counts and ratio for a categorical predictor (e.g., 'type').\n",
            "    \"\"\"\n",
            "    if cat_col not in df.columns:\n",
            "        return {'error': f\"{cat_col} not in df\"}\n",
            "    counts = df[cat_col].value_counts().to_dict()\n",
            "    total = len(df)\n",
            "    ratios = {k: v/total for k,v in counts.items()}\n",
            "    log = {'counts': counts, 'ratios': ratios}\n",
            "    return log\n",
            "\n",
            "# Helper 6: basic checks (nulls and dtypes)\n",
            "def basic_type_checks(df: pd.DataFrame) -> Dict:\n",
            "    \"\"\"\n",
            "    Return null counts and dtypes snapshot.\n",
            "    \"\"\"\n",
            "    nulls = df.isnull().sum().to_dict()\n",
            "    dtypes = {k: str(v) for k,v in df.dtypes.items()}\n",
            "    return {'null_counts': nulls, 'dtypes': dtypes}\n",
            "\n",
            "# Wrapper: calls helpers with boolean flags for user choice\n",
            "def preprocess_dataset(\n",
            "    csv_url: str,\n",
            "    remove_dups: bool = True,\n",
            "    outlier_method: str = 'cap',   # 'cap', 'remove', or 'none'\n",
            "    outlier_columns: List[str] = None,\n",
            "    outlier_cap_quantile: float = 0.01,\n",
            "    drop_correlated: bool = True,\n",
            "    corr_threshold: float = 0.70,\n",
            "    target_col: str = 'quality',\n",
            "    report_type_imbalance: bool = True\n",
            ") -> Tuple[pd.DataFrame, Dict]:\n",
            "    \"\"\"\n",
            "    Full preprocessing pipeline wrapper. Returns (clean_df, log_dict).\n",
            "    \"\"\"\n",
            "    log = {}\n",
            "    df = load_data_from_github(csv_url)\n",
            "    log['initial_shape'] = df.shape\n",
            "    # Basic checks\n",
            "    log['basic_checks'] = basic_type_checks(df)\n",
            "    # Remove duplicates\n",
            "    if remove_dups:\n",
            "        df, dup_log = remove_duplicates(df, drop=True)\n",
            "        log['duplicates'] = dup_log\n",
            "    # Outliers\n",
            "    df, out_log = report_and_handle_outliers_iqr(df, columns=outlier_columns, method=outlier_method, cap_quantile=outlier_cap_quantile)\n",
            "    log['outliers'] = out_log\n",
            "    # Drop correlated features\n",
            "    if drop_correlated:\n",
            "        df, corr_log = drop_highly_correlated(df, threshold=corr_threshold, target_col=target_col, verbose=False)\n",
            "        log['correlation_reduction'] = corr_log\n",
            "    # Type imbalance\n",
            "    if report_type_imbalance:\n",
            "        log['type_imbalance'] = flag_type_imbalance(df, 'type') if 'type' in df.columns else {'type': 'not present'}\n",
            "    log['final_shape'] = df.shape\n",
            "    return df, log\n",
            "\n",
            "# Example single-line run (replace GITHUB_CSV_URL with your actual raw CSV URL on GitHub)\n",
            "# Example:\n",
            "# GITHUB_CSV_URL = \"https://raw.githubusercontent.com/your_repo/path/to/winequality.csv\"\n",
            "# df_clean, preprocessing_log = preprocess_dataset(GITHUB_CSV_URL, remove_dups=True, outlier_method='cap', drop_correlated=True, corr_threshold=0.70)\n",
            "```\n",
            "\n",
            "Single line to run the overall wrapper (replace URL with your GitHub raw CSV URL):\n",
            "- df_clean, preprocessing_log = preprocess_dataset(\"GITHUB_CSV_URL\", remove_dups=True, outlier_method='cap', drop_correlated=True, corr_threshold=0.70)\n",
            "\n",
            "Notes on default choices and justification (mapping to dataset profile):\n",
            "- remove_duplicates=True by default because 1,177 duplicates (~18%) is large and will distort training/validation.\n",
            "- outlier_method='cap' default (winsorize) because many columns show a substantial number of IQR outliers (citric acid 509, volatile acidity 377, fixed acidity 357, etc.): capping reduces extreme influence while preserving sample size; choose 'remove' only if you can justify discarding those rows after domain review.\n",
            "- corr_threshold=0.70 default follows the standard rule-of-thumb and the profile shows free vs total sulfur dioxide at r=0.721 so that pair will be considered and one dropped automatically (the script drops the one with lower absolute correlation to quality).\n",
            "- The wrapper does not encode 'type' nor model; it only reports imbalance so you can decide later how to handle it (one-hot, target encoding, or tree-based native handling).\n",
            "\n",
            "If you want, I can:\n",
            "- Suggest a short checklist for domain review of rows with extreme residual sugar (max 65.8) and many citric-acid outliers (509) — e.g., inspect sample rows, inspect source/measurement units, check duplicates among outlier rows (maybe some duplicates explain extreme values).\n",
            "- Provide an alternative pipeline that uses transform (log) for strongly right-skewed columns (residual sugar) and show how this affects tree splitting (trees typically don't need log transforms, but it can help reduce spurious splits on extreme singletons).\n",
            "\n",
            "If you want the script adapted (for instance to cap only columns with >X outliers or to use percentile capping instead of IQR) tell me which behavior you prefer and I will modify the helper functions accordingly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revised from Copilot - Use this"
      ],
      "metadata": {
        "id": "B7WmzytAwhz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    instructions=\"\"\"\n",
        "You are an expert data scientist with extensive knowledge of tree-based models.\n",
        "Always justify recommendations using reasoning trace based ONLY on the dataset profile.\n",
        "\"\"\",\n",
        "    input=f\"\"\"\n",
        "Dataset info: {payload_text}\\n\n",
        "Questions:\\n\n",
        "1. Based on the dataset profile and context, how should I approach the modelling objective? Focus on problem framing aspects.\n",
        "2. What would be the most meaningful target for classification (standard vs premium)?\n",
        "3. What would be the most important metric for scoring in this classification setting?\n",
        "4. What are the top 3 most potentially important features? If not shown in profile, state 'Not shown in profile'.\n",
        "\n",
        "\n",
        "Explain why each is problematic. \\n\n",
        "Next: Provide a python script to handle the identified issues.\n",
        "Define one helper function for each issue.\n",
        "Then define a wrapper function that calls these helper with true false option as user choice\n",
        "Provide a single line of code to run the overall wrapper function.\n",
        "Do not encode categorical columns or model first.\n",
        "\"\"\")\n",
        "\n",
        "print(response.output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d5a2c3-2aa6-48cb-a0e2-b296ff7925d8",
        "id": "mDdPgmbFve5S"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Problem framing — how to approach modelling (based on the profile)\n",
            "- Goal and scope: The dataset contains physicochemical measurements and a numeric \"quality\" score (integers 3–9). The stated objective is to classify wines as \"standard vs premium\". That is a supervised binary classification derived from the numeric quality column (so we must create a binary target from quality).\n",
            "- Class definition: Convert quality into a binary label (premium vs standard) before modelling. The distribution summary indicates most wines sit around quality 5–6 (mean 5.818, 25/50/75 = 5/6/6). A natural decision boundary is higher-quality tail (e.g., quality >= 7) as \"premium\" and the rest as \"standard\" — this creates a minority premium class (justified below).\n",
            "- Data split and evaluation: Use holdout / cross-validation stratified by the binary target to preserve class ratios. Because duplicates (1,177 duplicate rows) are present, deduplicate before splitting to avoid leakage between train and test.\n",
            "- Preprocessing priorities (before modelling): deduplication, define the binary target, handle extreme outliers in predictors (many features show a substantial count of IQR outliers), and consider class imbalance strategies. Do not encode categorical columns at this preprocessing stage (as requested).\n",
            "Reasoning trace: dedup_count = 1,177 (profile), no missing values (so no imputation required), many features have large IQR-outlier counts (see outlier summary) that can distort thresholds or cause unstable splits early in tree growth if left unchecked; quality is concentrated around 5–6 so premium (> =7) will be a minority.\n",
            "\n",
            "2) Best target for classification (standard vs premium)\n",
            "- Recommended target: premium = quality >= 7, standard = quality <= 6.\n",
            "Why: The quality distribution measures (25/50/75 = 5/6/6; mean ≈ 5.82) indicate most wines are clustered at 5–6, so selecting 7+ isolates the higher-quality tail as \"premium\". This is consistent with typical wine-quality binarizations and yields a meaningful business distinction (higher sensory scores = premium). Reasoning trace: quality unique values = 7 (3–9), central tendency around 6, so threshold 7 clearly separates a higher-quality minority.\n",
            "\n",
            "3) Best metric for scoring\n",
            "- Primary metric: F1-score for the premium (positive) class (or the class-specific F1 for the premium class).\n",
            "Why: premium is expected to be the minority class (central tendency near 6); in imbalanced binary problems a single accuracy score is misleading. F1 for the premium class balances precision and recall — appropriate when both false positives (mislabeling standard as premium) and false negatives (missing premium wines) matter. Reasoning trace: quality mean ~5.82 and quartiles 5/6/6 imply premium(>=7) is a smaller group; use F1 for minority class. Also report ROC AUC and precision-recall AUC as secondary diagnostics.\n",
            "\n",
            "4) Top 3 potentially important features and why each is problematic (trace from profile)\n",
            "Select by absolute Pearson correlation with quality (from the profile's correlation matrix):\n",
            "- alcohol (corr with quality = +0.444)\n",
            "  - Why important: strongest positive correlation with quality in profile.\n",
            "  - Why problematic: there are some outliers in alcohol (outlier summary: 3 outliers) and alcohol is strongly negatively correlated with density (corr = -0.687). The alcohol–density relation may introduce redundancy or mask separate effects if not considered. Also if you later standardize or apply linear models this collinearity matters (for tree-based models it's less harmful, but it can influence split ordering).\n",
            "  - Reasoning trace: corr(alcohol, quality) = 0.444; outlier count (alcohol) = 3; corr(alcohol, density) = -0.687.\n",
            "- density (corr with quality = -0.306)\n",
            "  - Why important: second-largest absolute correlation to quality (negative).\n",
            "  - Why problematic: density has 3 outliers, and it is highly correlated with alcohol (-0.687) and moderately correlated with residual sugar (0.553). This multivariate correlation structure can complicate feature interpretation and may lead to split preference artifacts in tree-based models.\n",
            "  - Reasoning trace: corr(density, quality) = -0.306; outlier count (density) = 3; corr(density, alcohol) = -0.687; corr(density, residual sugar) = 0.553.\n",
            "- volatile acidity (corr with quality = -0.266)\n",
            "  - Why important: notable negative correlation with quality (higher volatile acidity -> lower quality).\n",
            "  - Why problematic: volatile acidity has many IQR outliers (377) and is correlated with other sulfur-related variables (e.g., total sulfur dioxide corr = -0.414, free sulfur dioxide corr = -0.353), which may create multicollinearity-like dependencies and heavy-tailed behavior that can affect model stability and interpretability.\n",
            "  - Reasoning trace: corr(volatile acidity, quality) = -0.266; outlier count (volatile acidity) = 377; corr(volatile acidity, total sulfur dioxide) = -0.414; corr(volatile acidity, free sulfur dioxide) = -0.353.\n",
            "\n",
            "Summary of other issues from the profile (that we should handle before modelling)\n",
            "- Duplicates: 1,177 duplicate rows — must be removed or at least deduplicated before train/test split.\n",
            "- Outliers: many features have substantial IQR outlier counts (citric acid 509, volatile acidity 377, fixed acidity 357, chlorides 286, sulphates 191, quality 228, etc.) — outlier handling needed to reduce extreme tail effects.\n",
            "- No missing values — no imputation needed.\n",
            "- Categorical imbalance in \"type\": white 4,898 vs red 1,599 — this may be predictive (but we will not encode it now). Reasoning trace: type counts from profile.\n",
            "\n",
            "Next: Python script to handle the identified issues\n",
            "- Per your request: define one helper function for each issue, then a wrapper that calls the helpers with boolean options. We will:\n",
            "  - remove duplicates,\n",
            "  - cap outliers by IQR per column,\n",
            "  - create the binary target (premium vs standard) using threshold (default >=7),\n",
            "  - optionally balance the target by simple random oversampling (if user asks),\n",
            "  - optionally drop highly collinear features (user choice).\n",
            "- We will not encode categorical columns or model.\n",
            "\n",
            "Script (copy/run in your environment). It expects a pandas DataFrame named wine_df (or pass any df). The helper functions return a modified DataFrame copy.\n",
            "\n",
            "```python\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "\n",
            "def remove_duplicates(df: pd.DataFrame, inplace: bool = False) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Remove exact duplicate rows.\n",
            "    Uses a copy by default. Returns deduplicated DataFrame.\n",
            "    Reason: profile shows 1,177 duplicate rows which can leak between train/test.\n",
            "    \"\"\"\n",
            "    if inplace:\n",
            "        df.drop_duplicates(inplace=True)\n",
            "        return df\n",
            "    return df.drop_duplicates()\n",
            "\n",
            "def cap_outliers_iqr(df: pd.DataFrame, cols: list = None, factor: float = 1.5, inplace: bool = False) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Cap numeric columns at (Q1 - factor*IQR, Q3 + factor*IQR).\n",
            "    Default excludes the 'quality' column so the original quality scores remain intact.\n",
            "    Returns DataFrame with values clipped to these bounds for each column.\n",
            "    Reason: many features (volatile acidity, citric acid, etc.) show many IQR outliers per profile.\n",
            "    \"\"\"\n",
            "    df_out = df if inplace else df.copy()\n",
            "    num_cols = df_out.select_dtypes(include=[np.number]).columns.tolist()\n",
            "    # do not cap the quality (keep original for target creation)\n",
            "    if 'quality' in num_cols:\n",
            "        num_cols.remove('quality')\n",
            "    if cols is None:\n",
            "        cols = num_cols\n",
            "    for col in cols:\n",
            "        if col not in df_out.columns:\n",
            "            continue\n",
            "        q1 = df_out[col].quantile(0.25)\n",
            "        q3 = df_out[col].quantile(0.75)\n",
            "        iqr = q3 - q1\n",
            "        lower = q1 - factor * iqr\n",
            "        upper = q3 + factor * iqr\n",
            "        # Only apply if there is variance\n",
            "        if pd.isna(lower) or pd.isna(upper):\n",
            "            continue\n",
            "        df_out[col] = df_out[col].clip(lower=lower, upper=upper)\n",
            "    return df_out\n",
            "\n",
            "def create_binary_target(df: pd.DataFrame, threshold: int = 7, target_name: str = 'is_premium', inplace: bool = False) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Create a binary target column: 1 if quality >= threshold else 0.\n",
            "    Default threshold = 7 (recommended since quality distribution centers at 5-6).\n",
            "    Returns DataFrame with new column.\n",
            "    \"\"\"\n",
            "    df_out = df if inplace else df.copy()\n",
            "    df_out[target_name] = (df_out['quality'] >= threshold).astype(int)\n",
            "    return df_out\n",
            "\n",
            "def simple_balance_oversample(df: pd.DataFrame, target_col: str = 'is_premium', random_state: int = 42, inplace: bool = False) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Simple random oversampling of minority class to match majority class count.\n",
            "    Returns balanced DataFrame.\n",
            "    Note: This is a basic, transparent option. More advanced resampling can be used later.\n",
            "    \"\"\"\n",
            "    df_out = df if inplace else df.copy()\n",
            "    counts = df_out[target_col].value_counts()\n",
            "    if len(counts) < 2:\n",
            "        return df_out\n",
            "    maj_count = counts.max()\n",
            "    parts = []\n",
            "    for cls, cnt in counts.items():\n",
            "        df_cls = df_out[df_out[target_col] == cls]\n",
            "        if cnt < maj_count:\n",
            "            df_cls_over = df_cls.sample(n=maj_count, replace=True, random_state=random_state)\n",
            "            parts.append(df_cls_over)\n",
            "        else:\n",
            "            parts.append(df_cls)\n",
            "    df_bal = pd.concat(parts, axis=0).sample(frac=1.0, random_state=random_state).reset_index(drop=True)\n",
            "    return df_bal\n",
            "\n",
            "def drop_collinear(df: pd.DataFrame, threshold: float = 0.95, exclude: list = None, inplace: bool = False) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Drop one variable from pairs with absolute Pearson correlation >= threshold.\n",
            "    Exclude specified columns (e.g., 'quality', target) from dropping.\n",
            "    Returns DataFrame with a reduced set of numeric columns.\n",
            "    Reason: optional cleanup if you want to remove near-duplicates; tree models are robust but this may help interpretation.\n",
            "    \"\"\"\n",
            "    df_out = df if inplace else df.copy()\n",
            "    if exclude is None:\n",
            "        exclude = []\n",
            "    num_cols = df_out.select_dtypes(include=[np.number]).columns.tolist()\n",
            "    cols_to_check = [c for c in num_cols if c not in exclude]\n",
            "    if len(cols_to_check) <= 1:\n",
            "        return df_out\n",
            "    corr = df_out[cols_to_check].corr().abs()\n",
            "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
            "    to_drop = [column for column in upper.columns if any(upper[column] >= threshold)]\n",
            "    if to_drop:\n",
            "        df_out = df_out.drop(columns=to_drop)\n",
            "    return df_out\n",
            "\n",
            "def preprocess(\n",
            "    df: pd.DataFrame,\n",
            "    remove_dup: bool = True,\n",
            "    cap_outliers: bool = True,\n",
            "    outlier_factor: float = 1.5,\n",
            "    create_target_flag: bool = True,\n",
            "    target_threshold: int = 7,\n",
            "    balance_target: bool = False,\n",
            "    balance_random_state: int = 42,\n",
            "    drop_collinear_flag: bool = False,\n",
            "    collinear_threshold: float = 0.95\n",
            ") -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Wrapper to run selected preprocessing steps (no categorical encoding, no modeling).\n",
            "    Steps (in order):\n",
            "      1) remove duplicates\n",
            "      2) cap outliers by IQR (excluding 'quality')\n",
            "      3) create binary target is_premium (quality >= threshold)\n",
            "      4) optionally balance target by random oversampling\n",
            "      5) optionally drop highly collinear numeric features (exclude 'quality' and 'is_premium')\n",
            "    Returns processed DataFrame copy.\n",
            "    \"\"\"\n",
            "    df_p = df.copy()\n",
            "    if remove_dup:\n",
            "        df_p = remove_duplicates(df_p, inplace=False)\n",
            "    if cap_outliers:\n",
            "        # cap all numeric columns except quality\n",
            "        df_p = cap_outliers_iqr(df_p, factor=outlier_factor, inplace=False)\n",
            "    if create_target_flag:\n",
            "        df_p = create_binary_target(df_p, threshold=target_threshold, target_name='is_premium', inplace=False)\n",
            "    if balance_target:\n",
            "        df_p = simple_balance_oversample(df_p, target_col='is_premium', random_state=balance_random_state, inplace=False)\n",
            "    if drop_collinear_flag:\n",
            "        exclude = ['quality', 'is_premium']\n",
            "        df_p = drop_collinear(df_p, threshold=collinear_threshold, exclude=exclude, inplace=False)\n",
            "    return df_p\n",
            "```\n",
            "\n",
            "Single-line code to run the overall wrapper (example):\n",
            "- If your raw DataFrame is named wine_df, run:\n",
            "```python\n",
            "processed_df = preprocess(wine_df, remove_dup=True, cap_outliers=True, outlier_factor=1.5, create_target_flag=True, target_threshold=7, balance_target=False, drop_collinear_flag=False)\n",
            "```\n",
            "\n",
            "Notes and recommended next steps (brief):\n",
            "- After this preprocessing, proceed to feature engineering and encoding (e.g., one-hot encode \"type\") and then model training. For tree-based models, capping outliers and deduplication will reduce noise and leakage; trees are robust to monotonic transforms, but very extreme outliers can still influence splits.\n",
            "- Use stratified CV and focus on premium-class F1 (and PR-AUC) as primary metric; report class-wise precision/recall too.\n",
            "- If you want to try balancing, set balance_target=True in the preprocess wrapper (or use more sophisticated resampling like SMOTE if desired).\n",
            "\n",
            "All recommendations and thresholds above are justified explicitly from the provided dataset profile (duplicates = 1,177; outlier counts per feature; correlation matrix values; quality distribution statistics).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def remove_duplicates(df: pd.DataFrame, inplace: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Remove exact duplicate rows.\n",
        "    Uses a copy by default. Returns deduplicated DataFrame.\n",
        "    Reason: profile shows 1,177 duplicate rows which can leak between train/test.\n",
        "    \"\"\"\n",
        "    if inplace:\n",
        "        df.drop_duplicates(inplace=True)\n",
        "        return df\n",
        "    return df.drop_duplicates()\n",
        "\n",
        "def cap_outliers_iqr(df: pd.DataFrame, cols: list = None, factor: float = 1.5, inplace: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cap numeric columns at (Q1 - factor*IQR, Q3 + factor*IQR).\n",
        "    Default excludes the 'quality' column so the original quality scores remain intact.\n",
        "    Returns DataFrame with values clipped to these bounds for each column.\n",
        "    Reason: many features (volatile acidity, citric acid, etc.) show many IQR outliers per profile.\n",
        "    \"\"\"\n",
        "    df_out = df if inplace else df.copy()\n",
        "    num_cols = df_out.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    # do not cap the quality (keep original for target creation)\n",
        "    if 'quality' in num_cols:\n",
        "        num_cols.remove('quality')\n",
        "    if cols is None:\n",
        "        cols = num_cols\n",
        "    for col in cols:\n",
        "        if col not in df_out.columns:\n",
        "            continue\n",
        "        q1 = df_out[col].quantile(0.25)\n",
        "        q3 = df_out[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower = q1 - factor * iqr\n",
        "        upper = q3 + factor * iqr\n",
        "        # Only apply if there is variance\n",
        "        if pd.isna(lower) or pd.isna(upper):\n",
        "            continue\n",
        "        df_out[col] = df_out[col].clip(lower=lower, upper=upper)\n",
        "    return df_out\n",
        "\n",
        "def create_binary_target(df: pd.DataFrame, threshold: int = 7, target_name: str = 'is_premium', inplace: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a binary target column: 1 if quality >= threshold else 0.\n",
        "    Default threshold = 7 (recommended since quality distribution centers at 5-6).\n",
        "    Returns DataFrame with new column.\n",
        "    \"\"\"\n",
        "    df_out = df if inplace else df.copy()\n",
        "    df_out[target_name] = (df_out['quality'] >= threshold).astype(int)\n",
        "    return df_out\n",
        "\n",
        "def simple_balance_oversample(df: pd.DataFrame, target_col: str = 'is_premium', random_state: int = 42, inplace: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Simple random oversampling of minority class to match majority class count.\n",
        "    Returns balanced DataFrame.\n",
        "    Note: This is a basic, transparent option. More advanced resampling can be used later.\n",
        "    \"\"\"\n",
        "    df_out = df if inplace else df.copy()\n",
        "    counts = df_out[target_col].value_counts()\n",
        "    if len(counts) < 2:\n",
        "        return df_out\n",
        "    maj_count = counts.max()\n",
        "    parts = []\n",
        "    for cls, cnt in counts.items():\n",
        "        df_cls = df_out[df_out[target_col] == cls]\n",
        "        if cnt < maj_count:\n",
        "            df_cls_over = df_cls.sample(n=maj_count, replace=True, random_state=random_state)\n",
        "            parts.append(df_cls_over)\n",
        "        else:\n",
        "            parts.append(df_cls)\n",
        "    df_bal = pd.concat(parts, axis=0).sample(frac=1.0, random_state=random_state).reset_index(drop=True)\n",
        "    return df_bal\n",
        "\n",
        "def drop_collinear(df: pd.DataFrame, threshold: float = 0.95, exclude: list = None, inplace: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Drop one variable from pairs with absolute Pearson correlation >= threshold.\n",
        "    Exclude specified columns (e.g., 'quality', target) from dropping.\n",
        "    Returns DataFrame with a reduced set of numeric columns.\n",
        "    Reason: optional cleanup if you want to remove near-duplicates; tree models are robust but this may help interpretation.\n",
        "    \"\"\"\n",
        "    df_out = df if inplace else df.copy()\n",
        "    if exclude is None:\n",
        "        exclude = []\n",
        "    num_cols = df_out.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cols_to_check = [c for c in num_cols if c not in exclude]\n",
        "    if len(cols_to_check) <= 1:\n",
        "        return df_out\n",
        "    corr = df_out[cols_to_check].corr().abs()\n",
        "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "    to_drop = [column for column in upper.columns if any(upper[column] >= threshold)]\n",
        "    if to_drop:\n",
        "        df_out = df_out.drop(columns=to_drop)\n",
        "    return df_out\n",
        "\n",
        "def preprocess(\n",
        "    df: pd.DataFrame,\n",
        "    remove_dup: bool = True,\n",
        "    cap_outliers: bool = True,\n",
        "    outlier_factor: float = 1.5,\n",
        "    create_target_flag: bool = True,\n",
        "    target_threshold: int = 7,\n",
        "    balance_target: bool = False,\n",
        "    balance_random_state: int = 42,\n",
        "    drop_collinear_flag: bool = False,\n",
        "    collinear_threshold: float = 0.95\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Wrapper to run selected preprocessing steps (no categorical encoding, no modeling).\n",
        "    Steps (in order):\n",
        "      1) remove duplicates\n",
        "      2) cap outliers by IQR (excluding 'quality')\n",
        "      3) create binary target is_premium (quality >= threshold)\n",
        "      4) optionally balance target by random oversampling\n",
        "      5) optionally drop highly collinear numeric features (exclude 'quality' and 'is_premium')\n",
        "    Returns processed DataFrame copy.\n",
        "    \"\"\"\n",
        "    df_p = df.copy()\n",
        "    if remove_dup:\n",
        "        df_p = remove_duplicates(df_p, inplace=False)\n",
        "    if cap_outliers:\n",
        "        # cap all numeric columns except quality\n",
        "        df_p = cap_outliers_iqr(df_p, factor=outlier_factor, inplace=False)\n",
        "    if create_target_flag:\n",
        "        df_p = create_binary_target(df_p, threshold=target_threshold, target_name='is_premium', inplace=False)\n",
        "    if balance_target:\n",
        "        df_p = simple_balance_oversample(df_p, target_col='is_premium', random_state=balance_random_state, inplace=False)\n",
        "    if drop_collinear_flag:\n",
        "        exclude = ['quality', 'is_premium']\n",
        "        df_p = drop_collinear(df_p, threshold=collinear_threshold, exclude=exclude, inplace=False)\n",
        "    return df_p"
      ],
      "metadata": {
        "id": "28s23r8qw57b"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# One of the possible response from GPT.\n",
        "# What are the differences with yours?\n",
        "# How would you improve earlier prompt?\n",
        "# ---------------------------\n",
        "\n",
        "# Helper 1: drop unique identifier / leakage column\n",
        "def drop_unique_identifier(df, col_name='Unnamed: 0', do_drop=True):\n",
        "    \"\"\"\n",
        "    Drops a column that is a unique identifier / possible leakage if do_drop is True.\n",
        "    Uses profile evidence: Unnamed: 0 has unique_count == nrows.\n",
        "    Returns (df_modified, info_dict).\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    info = {'dropped': [], 'skipped': []}\n",
        "    if col_name in df.columns:\n",
        "        if do_drop:\n",
        "            # safety: check uniqueness before dropping\n",
        "            if df[col_name].nunique() == len(df):\n",
        "                df = df.drop(columns=[col_name])\n",
        "                info['dropped'].append(col_name)\n",
        "            else:\n",
        "                # not unique, still allow drop if user insisted\n",
        "                df = df.drop(columns=[col_name])\n",
        "                info['dropped'].append(col_name + ' (not unique)')\n",
        "        else:\n",
        "            info['skipped'].append(col_name)\n",
        "    else:\n",
        "        info['skipped'].append(col_name + ' (not present)')\n",
        "    return df, info\n",
        "\n",
        "# Helper 2: drop constant columns\n",
        "def drop_constant_columns(df, do_drop=True):\n",
        "    \"\"\"\n",
        "    Drops columns that have a single unique value (constant) if do_drop is True.\n",
        "    Based on profile: Storage_Type is constant (unique == 1).\n",
        "    Returns (df_modified, info_dict).\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    info = {'dropped': [], 'skipped': []}\n",
        "    const_cols = [c for c in df.columns if df[c].nunique() <= 1]\n",
        "    if do_drop:\n",
        "        if const_cols:\n",
        "            df = df.drop(columns=const_cols)\n",
        "            info['dropped'].extend(const_cols)\n",
        "        else:\n",
        "            info['skipped'].append('no_constant_columns_found')\n",
        "    else:\n",
        "        info['skipped'].extend(const_cols)\n",
        "    return df, info\n",
        "\n",
        "# Helper 3: resolve multicollinearity between Weight_kg and Screen_Size_inch\n",
        "def resolve_weight_screen_collinearity(df, drop_screen=True):\n",
        "    \"\"\"\n",
        "    Resolves the strong collinearity (profile: corr = 0.924) between Weight_kg and Screen_Size_inch.\n",
        "    By default drops Screen_Size_inch and keeps Weight_kg (Weight has 244 unique values vs Screen_Size 6).\n",
        "    Set drop_screen=False to keep Screen_Size_inch and drop Weight_kg instead.\n",
        "    Returns (df_modified, info_dict).\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    info = {'dropped': [], 'kept': []}\n",
        "    w_col = 'Weight_kg'\n",
        "    s_col = 'Screen_Size_inch'\n",
        "    # Only act if both exist\n",
        "    if w_col in df.columns and s_col in df.columns:\n",
        "        if drop_screen:\n",
        "            df = df.drop(columns=[s_col])\n",
        "            info['dropped'].append(s_col)\n",
        "            info['kept'].append(w_col)\n",
        "        else:\n",
        "            df = df.drop(columns=[w_col])\n",
        "            info['dropped'].append(w_col)\n",
        "            info['kept'].append(s_col)\n",
        "    else:\n",
        "        # if one is missing, note what remains\n",
        "        if w_col in df.columns:\n",
        "            info['kept'].append(w_col)\n",
        "        if s_col in df.columns:\n",
        "            info['kept'].append(s_col)\n",
        "    return df, info\n",
        "\n",
        "# Helper 4: optional: drop Model column (high-cardinality / redundancy risk)\n",
        "def drop_model_column(df, do_drop=False):\n",
        "    \"\"\"\n",
        "    Drops the Model column if do_drop is True.\n",
        "    Reason: Model has 30 unique values and can encode product-specific pricing patterns (risk of overfitting).\n",
        "    Caller can choose to drop or keep.\n",
        "    Returns (df_modified, info_dict).\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    info = {'dropped': [], 'skipped': []}\n",
        "    col = 'Model'\n",
        "    if col in df.columns:\n",
        "        if do_drop:\n",
        "            df = df.drop(columns=[col])\n",
        "            info['dropped'].append(col)\n",
        "        else:\n",
        "            info['skipped'].append(col)\n",
        "    else:\n",
        "        info['skipped'].append(col + ' (not present)')\n",
        "    return df, info\n",
        "\n",
        "# Wrapper function that calls the helpers based on user boolean choices\n",
        "# User can edit the default parameters here as required\n",
        "def clean_data(\n",
        "    df,\n",
        "    drop_id=True,\n",
        "    drop_constants=True,\n",
        "    resolve_collinearity=False, #<--- slight differences and high similarity between columns are not always an issue\n",
        "    drop_model=False,\n",
        "    id_col='Unnamed: 0',\n",
        "    drop_screen_by_default=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Clean dataset according to the profile-driven actions.\n",
        "    Parameters:\n",
        "      - df: input DataFrame\n",
        "      - drop_id: drop the unique identifier (Unnamed: 0) if True\n",
        "      - drop_constants: drop constant columns (e.g., Storage_Type) if True\n",
        "      - resolve_collinearity: resolve Weight_kg vs Screen_Size_inch if True\n",
        "      - drop_model: drop Model column if True (optional decision)\n",
        "      - id_col: name of the identifier column (default 'Unnamed: 0')\n",
        "      - drop_screen_by_default: if resolving collinearity, drop Screen_Size_inch if True (keeps Weight_kg).\n",
        "    Returns:\n",
        "      - cleaned_df: DataFrame after applied changes\n",
        "      - summary: dict summarizing actions taken\n",
        "    \"\"\"\n",
        "    summary = {}\n",
        "    df_work = df.copy()\n",
        "    # 1. drop unique id\n",
        "    df_work, info1 = drop_unique_identifier(df_work, col_name=id_col, do_drop=drop_id)\n",
        "    summary['unique_id'] = info1\n",
        "    # 2. drop constant columns\n",
        "    df_work, info2 = drop_constant_columns(df_work, do_drop=drop_constants)\n",
        "    summary['constant_columns'] = info2\n",
        "    # 3. resolve strong collinearity\n",
        "    if resolve_collinearity:\n",
        "        df_work, info3 = resolve_weight_screen_collinearity(df_work, drop_screen=drop_screen_by_default)\n",
        "    else:\n",
        "        info3 = {'dropped': [], 'kept': []}\n",
        "    summary['collinearity'] = info3\n",
        "    # 4. drop model optionally\n",
        "    df_work, info4 = drop_model_column(df_work, do_drop=drop_model)\n",
        "    summary['model_column'] = info4\n",
        "\n",
        "    return df_work, summary\n",
        "\n",
        "# Example single-line execution (assuming your DataFrame is named `df`)\n",
        "# This default call: drops Unnamed: 0, drops constant columns (Storage_Type), drops Screen_Size_inch (keeps Weight_kg), and keeps Model.\n",
        "# To drop Model as well, set drop_model=True.\n",
        "cleaned_df, cleaning_summary = clean_data(df)"
      ],
      "metadata": {
        "id": "YjdEz0IPCGBM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Define Target and Features\n",
        "# ---------------------------\n",
        "\n",
        "# Target: classify wines into 'Standard' vs 'Premium'\n",
        "# Business framing: Premium wines are those with quality >= 7\n",
        "cleaned_df['target_quality'] = np.where(cleaned_df['quality'] >= 7, 'Premium', 'Standard')\n",
        "\n",
        "# y = target column\n",
        "y = cleaned_df['target_quality']\n",
        "\n",
        "# X = all other features except target and original quality column\n",
        "X = cleaned_df.drop(columns=['quality', 'target_quality'])"
      ],
      "metadata": {
        "id": "IH6x3c5vxwDS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "bC3q5tKO0LPs",
        "outputId": "84df3c90-2159-474e-b524-c862271c0048",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     type  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
              "0   white            7.0              0.27         0.36           20.70   \n",
              "1   white            6.3              0.30         0.34            1.60   \n",
              "2   white            8.1              0.28         0.40            6.90   \n",
              "3   white            7.2              0.23         0.32            8.50   \n",
              "4   white            7.2              0.23         0.32            8.50   \n",
              "5   white            8.1              0.28         0.40            6.90   \n",
              "6   white            6.2              0.32         0.16            7.00   \n",
              "7   white            7.0              0.27         0.36           20.70   \n",
              "8   white            6.3              0.30         0.34            1.60   \n",
              "9   white            8.1              0.22         0.43            1.50   \n",
              "10  white            8.1              0.27         0.41            1.45   \n",
              "11  white            8.6              0.23         0.40            4.20   \n",
              "12  white            7.9              0.18         0.37            1.20   \n",
              "13  white            6.6              0.16         0.40            1.50   \n",
              "14  white            8.3              0.42         0.62           19.25   \n",
              "15  white            6.6              0.17         0.38            1.50   \n",
              "16  white            6.3              0.48         0.04            1.10   \n",
              "17  white            6.2              0.66         0.48            1.20   \n",
              "18  white            7.4              0.34         0.42            1.10   \n",
              "19  white            6.5              0.31         0.14            7.50   \n",
              "\n",
              "    chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
              "0       0.045                 45.0                 170.0   1.0010  3.00   \n",
              "1       0.049                 14.0                 132.0   0.9940  3.30   \n",
              "2       0.050                 30.0                  97.0   0.9951  3.26   \n",
              "3       0.058                 47.0                 186.0   0.9956  3.19   \n",
              "4       0.058                 47.0                 186.0   0.9956  3.19   \n",
              "5       0.050                 30.0                  97.0   0.9951  3.26   \n",
              "6       0.045                 30.0                 136.0   0.9949  3.18   \n",
              "7       0.045                 45.0                 170.0   1.0010  3.00   \n",
              "8       0.049                 14.0                 132.0   0.9940  3.30   \n",
              "9       0.044                 28.0                 129.0   0.9938  3.22   \n",
              "10      0.033                 11.0                  63.0   0.9908  2.99   \n",
              "11      0.035                 17.0                 109.0   0.9947  3.14   \n",
              "12      0.040                 16.0                  75.0   0.9920  3.18   \n",
              "13      0.044                 48.0                 143.0   0.9912  3.54   \n",
              "14      0.040                 41.0                 172.0   1.0002  2.98   \n",
              "15      0.032                 28.0                 112.0   0.9914  3.25   \n",
              "16      0.046                 30.0                  99.0   0.9928  3.24   \n",
              "17      0.029                 29.0                  75.0   0.9892  3.33   \n",
              "18      0.033                 17.0                 171.0   0.9917  3.12   \n",
              "19      0.044                 34.0                 133.0   0.9955  3.22   \n",
              "\n",
              "    sulphates  alcohol  \n",
              "0        0.45      8.8  \n",
              "1        0.49      9.5  \n",
              "2        0.44     10.1  \n",
              "3        0.40      9.9  \n",
              "4        0.40      9.9  \n",
              "5        0.44     10.1  \n",
              "6        0.47      9.6  \n",
              "7        0.45      8.8  \n",
              "8        0.49      9.5  \n",
              "9        0.45     11.0  \n",
              "10       0.56     12.0  \n",
              "11       0.53      9.7  \n",
              "12       0.63     10.8  \n",
              "13       0.52     12.4  \n",
              "14       0.67      9.7  \n",
              "15       0.55     11.4  \n",
              "16       0.36      9.6  \n",
              "17       0.39     12.8  \n",
              "18       0.53     11.3  \n",
              "19       0.50      9.5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00d5dbfa-c0e7-4322-8f37-18e2b94cc710\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>white</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.70</td>\n",
              "      <td>0.045</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>1.0010</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>white</td>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.049</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.9940</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>white</td>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.90</td>\n",
              "      <td>0.050</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.9951</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>white</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.50</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>white</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.50</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>white</td>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.90</td>\n",
              "      <td>0.050</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.9951</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>white</td>\n",
              "      <td>6.2</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.16</td>\n",
              "      <td>7.00</td>\n",
              "      <td>0.045</td>\n",
              "      <td>30.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>0.9949</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.47</td>\n",
              "      <td>9.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>white</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.70</td>\n",
              "      <td>0.045</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>1.0010</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>white</td>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.049</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.9940</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>white</td>\n",
              "      <td>8.1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.044</td>\n",
              "      <td>28.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>0.9938</td>\n",
              "      <td>3.22</td>\n",
              "      <td>0.45</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>white</td>\n",
              "      <td>8.1</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.41</td>\n",
              "      <td>1.45</td>\n",
              "      <td>0.033</td>\n",
              "      <td>11.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>0.9908</td>\n",
              "      <td>2.99</td>\n",
              "      <td>0.56</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>white</td>\n",
              "      <td>8.6</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.40</td>\n",
              "      <td>4.20</td>\n",
              "      <td>0.035</td>\n",
              "      <td>17.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>0.9947</td>\n",
              "      <td>3.14</td>\n",
              "      <td>0.53</td>\n",
              "      <td>9.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>white</td>\n",
              "      <td>7.9</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.37</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.040</td>\n",
              "      <td>16.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>0.9920</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.63</td>\n",
              "      <td>10.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>white</td>\n",
              "      <td>6.6</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.40</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.044</td>\n",
              "      <td>48.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>0.9912</td>\n",
              "      <td>3.54</td>\n",
              "      <td>0.52</td>\n",
              "      <td>12.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>white</td>\n",
              "      <td>8.3</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.62</td>\n",
              "      <td>19.25</td>\n",
              "      <td>0.040</td>\n",
              "      <td>41.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>2.98</td>\n",
              "      <td>0.67</td>\n",
              "      <td>9.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>white</td>\n",
              "      <td>6.6</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.38</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.032</td>\n",
              "      <td>28.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.9914</td>\n",
              "      <td>3.25</td>\n",
              "      <td>0.55</td>\n",
              "      <td>11.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>white</td>\n",
              "      <td>6.3</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.04</td>\n",
              "      <td>1.10</td>\n",
              "      <td>0.046</td>\n",
              "      <td>30.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>0.9928</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.36</td>\n",
              "      <td>9.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>white</td>\n",
              "      <td>6.2</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.48</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.029</td>\n",
              "      <td>29.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>0.9892</td>\n",
              "      <td>3.33</td>\n",
              "      <td>0.39</td>\n",
              "      <td>12.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>white</td>\n",
              "      <td>7.4</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.42</td>\n",
              "      <td>1.10</td>\n",
              "      <td>0.033</td>\n",
              "      <td>17.0</td>\n",
              "      <td>171.0</td>\n",
              "      <td>0.9917</td>\n",
              "      <td>3.12</td>\n",
              "      <td>0.53</td>\n",
              "      <td>11.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>white</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.14</td>\n",
              "      <td>7.50</td>\n",
              "      <td>0.044</td>\n",
              "      <td>34.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>0.9955</td>\n",
              "      <td>3.22</td>\n",
              "      <td>0.50</td>\n",
              "      <td>9.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00d5dbfa-c0e7-4322-8f37-18e2b94cc710')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-00d5dbfa-c0e7-4322-8f37-18e2b94cc710 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-00d5dbfa-c0e7-4322-8f37-18e2b94cc710');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X",
              "summary": "{\n  \"name\": \"X\",\n  \"rows\": 6497,\n  \"fields\": [\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"red\",\n          \"white\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fixed acidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.296433757799818,\n        \"min\": 3.8,\n        \"max\": 15.9,\n        \"num_unique_values\": 106,\n        \"samples\": [\n          14.3,\n          6.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"volatile acidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16463647408467888,\n        \"min\": 0.08,\n        \"max\": 1.58,\n        \"num_unique_values\": 187,\n        \"samples\": [\n          0.895,\n          0.655\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"citric acid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1453178648975926,\n        \"min\": 0.0,\n        \"max\": 1.66,\n        \"num_unique_values\": 89,\n        \"samples\": [\n          0.45,\n          0.51\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"residual sugar\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.7578037431474005,\n        \"min\": 0.6,\n        \"max\": 65.8,\n        \"num_unique_values\": 316,\n        \"samples\": [\n          14.7,\n          4.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chlorides\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.035033601372459124,\n        \"min\": 0.009,\n        \"max\": 0.611,\n        \"num_unique_values\": 214,\n        \"samples\": [\n          0.046,\n          0.387\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"free sulfur dioxide\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.749399772002565,\n        \"min\": 1.0,\n        \"max\": 289.0,\n        \"num_unique_values\": 135,\n        \"samples\": [\n          146.5,\n          64.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total sulfur dioxide\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56.52185452263032,\n        \"min\": 6.0,\n        \"max\": 440.0,\n        \"num_unique_values\": 276,\n        \"samples\": [\n          158.0,\n          194.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"density\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002998673003719037,\n        \"min\": 0.98711,\n        \"max\": 1.03898,\n        \"num_unique_values\": 998,\n        \"samples\": [\n          0.99144,\n          0.99734\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pH\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16078720210398814,\n        \"min\": 2.72,\n        \"max\": 4.01,\n        \"num_unique_values\": 108,\n        \"samples\": [\n          2.74,\n          3.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sulphates\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14880587361449032,\n        \"min\": 0.22,\n        \"max\": 2.0,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          1.08,\n          0.55\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"alcohol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1927117488689747,\n        \"min\": 8.0,\n        \"max\": 14.9,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          12.33333333,\n          11.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "6HPv9Ue50aAY",
        "outputId": "9bab0fb4-c03f-408b-c31d-773eebc42bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     Standard\n",
              "1     Standard\n",
              "2     Standard\n",
              "3     Standard\n",
              "4     Standard\n",
              "5     Standard\n",
              "6     Standard\n",
              "7     Standard\n",
              "8     Standard\n",
              "9     Standard\n",
              "10    Standard\n",
              "11    Standard\n",
              "12    Standard\n",
              "13     Premium\n",
              "14    Standard\n",
              "15     Premium\n",
              "16    Standard\n",
              "17     Premium\n",
              "18    Standard\n",
              "19    Standard\n",
              "Name: target_quality, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target_quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Premium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Premium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Premium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Standard</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "487928b5"
      },
      "source": [
        "\n",
        "## Chapter 7. Baseline modelling: train/test split to create a holdout set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "913bb1b4"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#define cat and num columns\n",
        "cat_columns=X.select_dtypes(include='object').columns\n",
        "num_columns=X.select_dtypes(exclude='object').columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ---------------------------\n",
        "# Baseline Modelling: Train/Test Split\n",
        "# ---------------------------\n",
        "\n",
        "# Split into train and test sets\n",
        "# Stratify ensures class balance (important for Standard vs Premium wines)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,       # preserve class distribution\n",
        "    random_state=42   # reproducibility\n",
        ")\n",
        "\n",
        "print(\"Training set size:\", X_train.shape[0])\n",
        "print(\"Test set size:\", X_test.shape[0])\n",
        "print(\"Class distribution in train:\\n\", y_train.value_counts(normalize=True))\n",
        "print(\"Class distribution in test:\\n\", y_test.value_counts(normalize=True))\n",
        "\n",
        "# ---------------------------\n",
        "# Define categorical and numeric columns\n",
        "# ---------------------------\n",
        "\n",
        "cat_columns = X.select_dtypes(include='object').columns\n",
        "num_columns = X.select_dtypes(exclude='object').columns\n",
        "\n",
        "print(\"Categorical columns:\", cat_columns.tolist())\n",
        "print(\"Numeric columns:\", num_columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0Sx0GET1D6r",
        "outputId": "0d1ac8bf-ec1e-4208-d5b1-a5edd0fe454a",
        "collapsed": true
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 5197\n",
            "Test set size: 1300\n",
            "Class distribution in train:\n",
            " target_quality\n",
            "Standard    0.803541\n",
            "Premium     0.196459\n",
            "Name: proportion, dtype: float64\n",
            "Class distribution in test:\n",
            " target_quality\n",
            "Standard    0.803077\n",
            "Premium     0.196923\n",
            "Name: proportion, dtype: float64\n",
            "Categorical columns: ['type']\n",
            "Numeric columns: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "l_Eqb8AKGeiU"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
        "#categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1)\n",
        "#numeric_transformer = StandardScaler()\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "        (\"cat\", categorical_transformer, cat_columns)\n",
        "    ],remainder='passthrough')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, ShuffleSplit, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ],
      "metadata": {
        "id": "373AS7LXGxoH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The `model_summary` variable is already defined from the `results` DataFrame in the notebook state.\n",
        "# Let's construct the full user content for the LLM based on the original intent of the `prompt` variable.\n",
        "model_summary = results.to_string() # Define model_summary from the results DataFrame\n",
        "full_user_content = f\"\"\"\n",
        "You are a machine learning strategist helping a wine classification project.\n",
        "\n",
        "Here is the model performance summary:\n",
        "{model_summary}\n",
        "\n",
        "Please:\n",
        "1. Generate insights about model performance and generalization.\n",
        "2. Recommend the next course of action (e.g., tuning, feature engineering, ensembling).\n",
        "3. Suggest how to communicate these findings to stakeholders.\n",
        "4. Generate the python code to implement the next action.\n",
        "\n",
        "Respond in a clear, structured format.\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"\"\"\n",
        "You are an expert data scientist with extensive knowledge of tree-based models.\n",
        "Always justify recommendations using reasoning trace based ONLY on the dataset profile.\n",
        "\"\"\"},\n",
        "        {\"role\": \"user\", \"content\": full_user_content}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39cc6fd3-e5ac-4267-d6cf-ebea48751cd1",
        "id": "8cYrDhc4EhZX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of what I inspected (dataset profile = the model performance table and best params you supplied)\n",
            "- Training CV scores (cv_f1_train) are 1.0 for all three models (Decision Tree, Random Forest, XGBoost). This is perfect training fit.\n",
            "- Cross-validation validation F1 (cv_f1_val) is substantially lower: 0.744 (DT), 0.794 (RF), 0.801 (XGB). So models do not generalize from train → CV.\n",
            "- Holdout F1 is even lower: 0.564 (DT), 0.656 (RF), 0.654 (XGB). All models lose performance on the held-out test set.\n",
            "- CV standard deviations are small (≈0.01–0.02), so CV performance is stable across folds but optimistic relative to holdout.\n",
            "- Best hyperparameters show very high model complexity (DT max_depth=None, min_samples_split=2; RF max_depth=None; XGB max_depth=10, learning_rate=0.2, n_estimators=200).\n",
            "\n",
            "1) Insights about model performance and generalization (with reasoning trace tied to the dataset profile)\n",
            "- Severe overfitting: train F1 = 1.0 while cv_f1_val ≈ 0.74–0.80 and holdout F1 ≈ 0.56–0.66. Reasoning: perfect training scores and large gaps to validation/holdout indicate the models memorize training data.\n",
            "- CV appears optimistic relative to holdout: cv_f1_val - holdout_f1 gaps: DT ≈ 0.18, RF ≈ 0.14, XGB ≈ 0.15. Reasoning: stable CV (low cv_std) yet consistent drop to holdout suggests either (a) data leakage in CV or preprocessing, (b) distribution shift between CV folds and holdout, or (c) hyperparameters tuned on CV overfit to CV folds.\n",
            "- Complexity-driven overfitting: chosen hyperparameters (unbounded tree depth for DT/RF, deep XGB with lr=0.2) favor high variance models—consistent with perfect training F1 and large generalization gaps.\n",
            "- XGBoost/RF give the best CV F1 (≈0.80) and similar holdout (~0.65). Reasoning: both ensembles reduce variance compared to a single DT (DT holdout = 0.564), but ensembles still overfit because of unregularized tree complexity.\n",
            "- CV stability implies that cross-validation splitting is consistent (small cv_std), but stability does not guarantee that CV matches real-world holdout distribution.\n",
            "\n",
            "2) Recommended next course of action (ordered, with justification from the profile)\n",
            "Primary objectives: eliminate leakage or sampling problems first, then reduce model variance through regularization and robust evaluation.\n",
            "\n",
            "A. Investigate data / CV strategy and possible leakage (first, mandatory)\n",
            "- Why: consistent, sizable drop from CV → holdout (0.14–0.18) plus perfect train F1 strongly suggests either leakage or a mismatch between how CV was done and how holdout was created.\n",
            "- Actions: confirm holdout was held-out before any preprocessing (scaling, encoding, feature creation), ensure no target-derived features leaked into training, verify stratification/grouping used in CV matches holdout sampling (time-based or group-based splits if appropriate).\n",
            "- Expected effect: if leakage is present, fixing it will typically reduce CV performance but align CV and holdout; if distribution mismatch exists, you will identify root cause and either re-sample or change evaluation.\n",
            "\n",
            "B. Apply stronger model regularization + robust validation (second)\n",
            "- Why: best params show high complexity (max_depth=None, max_depth=10, lr=0.2). Regularization should reduce the train → CV gap and improve holdout performance.\n",
            "- Specific recommendations:\n",
            "  - For tree models: limit max_depth (3–6), increase min_samples_leaf (>=5–20), increase min_samples_split, and/or limit number of leaves.\n",
            "  - For Random Forest: reduce tree depth, increase min_samples_leaf, consider fewer correlated features via max_features < 1.\n",
            "  - For XGBoost: lower learning_rate (0.01–0.1), increase reg_alpha/reg_lambda, use subsample/colsample_bytree (0.6–0.9), use early_stopping_rounds with a validation set.\n",
            "- Use stratified (or group/time-aware) K-Fold and a nested CV or a separate validation fold for hyperparameter tuning to avoid overfitting to CV folds.\n",
            "- Expected effect: reduce variance, produce models whose CV F1 and holdout F1 converge closer.\n",
            "\n",
            "C. Feature inspection and light feature engineering (third)\n",
            "- Why: If a few features dominate prediction or are proxies for the target (leakage or overly predictive engineered features), models will overfit. Feature engineering can produce more robust signals and reduce over-reliance on noisy/leaky features.\n",
            "- Actions: compute feature importances and permutation importances on holdout; check for extremely high correlations with target, data leakage (IDs, timestamps), duplicated rows, and class imbalance issues.\n",
            "- Expected effect: removing/transforming problematic features reduces overfitting risk and can improve holdout performance.\n",
            "\n",
            "D. Only after A–C: consider ensembling and stacking (later)\n",
            "- Why: ensembling can improve performance but will not fix leakage or a flawed CV/feature pipeline. Ensembling over overfit models will still generalize poorly.\n",
            "- Actions: once models are regularized and validated with proper CV, try simple model averaging, soft-voting ensembles, or stacking with strict CV procedures.\n",
            "\n",
            "3) How to communicate these findings to stakeholders (concise, non-technical + recommended next steps and timeline)\n",
            "- Key message (one-line): \"Current models show strong fit on training data but substantially worse performance on truly held-out data — this indicates overfitting and/or a data/validation problem that we must fix before improving accuracy.\"\n",
            "- Short supporting evidence: \"Training F1 is 1.00, cross-validation F1 ≈ 0.80, holdout F1 ≈ 0.65 — the gap (≈0.15) is large and consistent across models.\"\n",
            "- Recommended plan and impact:\n",
            "  1. Immediately audit the data pipeline and validation splits (2–3 days) — this can reveal leaks or sampling issues that, when fixed, will make evaluation reliable.\n",
            "  2. Run a controlled regularization and evaluation experiment with XGBoost and RandomForest (1 week) — expect holdout F1 to rise if overfitting is reduced.\n",
            "  3. If results stabilize, pursue feature engineering and cautious ensembling (1–2 weeks).\n",
            "- Risk & expectation: \"If the issue is distribution shift between training and production data, performance gains may require collecting more representative data or retraining on new data. If the issue is leakage, fixing it may reduce some reported CV numbers but produce reliable real-world performance.\"\n",
            "- Deliverables: \"I'll provide (a) a short audit report of data/validation issues, (b) a reproducible experiment notebook with regularized models and holdout evaluation, and (c) a list of top features and recommendations for production monitoring.\"\n",
            "\n",
            "4) Python code to implement the next action\n",
            "I recommend as the immediate next action to (1) re-run model selection for XGBoost with stronger regularization and proper validation (StratifiedKFold), using early stopping on a validation fold inside each CV split, (2) log train/val scores so you can detect overfitting, and (3) compute permutation importances on the holdout to surface potential leakage. The following code implements this workflow. Replace placeholder variables (X, y) with your dataset and set aside a holdout (X_holdout, y_holdout) if not already available.\n",
            "\n",
            "Note: this code assumes you have X (features) and y (target) as numpy arrays or pandas DataFrame/Series. The code uses XGBClassifier from xgboost and scikit-learn. It runs a randomized search over conservative (regularized) hyperparameters and uses early stopping inside each CV fold.\n",
            "\n",
            "Python code (copy/paste)\n",
            "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\n",
            "from sklearn.metrics import f1_score, make_scorer\n",
            "from sklearn.inspection import permutation_importance\n",
            "from xgboost import XGBClassifier\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import time\n",
            "import warnings\n",
            "warnings.filterwarnings(\"ignore\")\n",
            "\n",
            "# --------------------------\n",
            "# Replace or prepare data\n",
            "# --------------------------\n",
            "# X, y -> full dataset\n",
            "# If you already have a holdout split, set X_train_all/y_train_all and X_holdout/y_holdout accordingly.\n",
            "# Otherwise we create a single holdout split here (stratified).\n",
            "# Example:\n",
            "# X = df.drop(columns=['target'])\n",
            "# y = df['target']\n",
            "\n",
            "# If you already have holdout:\n",
            "# X_train_all, y_train_all = X_train, y_train\n",
            "# X_holdout, y_holdout = X_holdout, y_holdout\n",
            "\n",
            "# Otherwise split:\n",
            "X_train_all, X_holdout, y_train_all, y_holdout = train_test_split(\n",
            "    X, y, test_size=0.15, stratify=y, random_state=42\n",
            ")\n",
            "\n",
            "# --------------------------\n",
            "# CV + RandomizedSearch with early stopping inside each CV fold\n",
            "# --------------------------\n",
            "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
            "f1_scorer = make_scorer(f1_score)\n",
            "\n",
            "# Conservative / regularized hyperparameter search space\n",
            "param_dist = {\n",
            "    'max_depth': [3, 4, 5, 6],\n",
            "    'learning_rate': [0.01, 0.03, 0.05, 0.08, 0.1],\n",
            "    'n_estimators': [100, 300, 800],\n",
            "    'subsample': [0.6, 0.7, 0.8, 1.0],\n",
            "    'colsample_bytree': [0.6, 0.7, 0.8, 1.0],\n",
            "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
            "    'reg_lambda': [1, 3, 5, 10],\n",
            "    'min_child_weight': [1, 3, 5]\n",
            "}\n",
            "\n",
            "# Wrapper model; we will perform a RandomizedSearchCV but use a custom fit approach\n",
            "# to allow early stopping within each fold. We'll implement a lightweight manual CV loop\n",
            "# that uses RandomizedSearch sampling for candidate params to keep things reproducible.\n",
            "\n",
            "n_iter = 30  # number of sampled parameter combinations\n",
            "rng = np.random.RandomState(42)\n",
            "param_list = []\n",
            "# sample random combinations\n",
            "keys = list(param_dist.keys())\n",
            "for _ in range(n_iter):\n",
            "    sample = {k: rng.choice(param_dist[k]) for k in keys}\n",
            "    param_list.append(sample)\n",
            "\n",
            "best_result = {'f1_mean': -np.inf}\n",
            "results = []\n",
            "\n",
            "start_time = time.time()\n",
            "for i, params in enumerate(param_list):\n",
            "    val_scores = []\n",
            "    train_scores = []\n",
            "    # Use early stopping with a small validation split inside each CV fold\n",
            "    for train_idx, val_idx in skf.split(X_train_all, y_train_all):\n",
            "        X_tr, X_val = X_train_all.iloc[train_idx], X_train_all.iloc[val_idx] if hasattr(X_train_all, \"iloc\") else X_train_all[train_idx], X_train_all[val_idx]\n",
            "        y_tr, y_val = y_train_all.iloc[train_idx], y_train_all.iloc[val_idx] if hasattr(y_train_all, \"iloc\") else y_train_all[train_idx], y_train_all[val_idx]\n",
            "        # create model with params + fixed n_jobs and random_state\n",
            "        model = XGBClassifier(\n",
            "            use_label_encoder=False,\n",
            "            eval_metric='logloss',\n",
            "            n_jobs=-1,\n",
            "            random_state=42,\n",
            "            **params\n",
            "        )\n",
            "        # fit with early stopping using the fold validation set\n",
            "        model.fit(\n",
            "            X_tr, y_tr,\n",
            "            eval_set=[(X_val, y_val)],\n",
            "            early_stopping_rounds=30,\n",
            "            verbose=False\n",
            "        )\n",
            "        # predict and score\n",
            "        y_val_pred = model.predict(X_val)\n",
            "        y_tr_pred = model.predict(X_tr)\n",
            "        val_scores.append(f1_score(y_val, y_val_pred))\n",
            "        train_scores.append(f1_score(y_tr, y_tr_pred))\n",
            "    mean_val = np.mean(val_scores)\n",
            "    mean_train = np.mean(train_scores)\n",
            "    results.append({'params': params, 'val_mean': mean_val, 'train_mean': mean_train, 'val_std': np.std(val_scores)})\n",
            "    # keep best\n",
            "    if mean_val > best_result['f1_mean']:\n",
            "        best_result = {'f1_mean': mean_val, 'params': params, 'val_scores': val_scores, 'train_scores': train_scores}\n",
            "    # optional: early stop search if we reach a performance target\n",
            "    # if best_result['f1_mean'] > 0.78:\n",
            "    #     break\n",
            "\n",
            "elapsed = time.time() - start_time\n",
            "print(f\"Randomized search completed in {elapsed:.1f}s. Best CV mean F1: {best_result['f1_mean']:.4f}\")\n",
            "print(\"Best params:\", best_result['params'])\n",
            "\n",
            "# Train final model on full training data with early stopping using a small internal validation split\n",
            "# We'll reserve 10% of X_train_all as a validation set\n",
            "X_tr_full, X_val_full, y_tr_full, y_val_full = train_test_split(\n",
            "    X_train_all, y_train_all, test_size=0.10, stratify=y_train_all, random_state=42\n",
            ")\n",
            "\n",
            "final_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_jobs=-1, random_state=42, **best_result['params'])\n",
            "final_model.fit(X_tr_full, y_tr_full, eval_set=[(X_val_full, y_val_full)], early_stopping_rounds=50, verbose=False)\n",
            "\n",
            "# Evaluate on holdout\n",
            "y_holdout_pred = final_model.predict(X_holdout)\n",
            "holdout_f1 = f1_score(y_holdout, y_holdout_pred)\n",
            "print(f\"Holdout F1 for final model: {holdout_f1:.4f}\")\n",
            "\n",
            "# Compare to previous holdout baseline if available (you can print baseline values)\n",
            "# print(\"Baseline holdout F1s: DT=0.5643, RF=0.6561, XGB=0.6536\")\n",
            "\n",
            "# Compute permutation importances on holdout to detect features that may be proxies/leaky\n",
            "perm = permutation_importance(final_model, X_holdout, y_holdout, scoring='f1', n_repeats=20, random_state=42, n_jobs=-1)\n",
            "importances = perm.importances_mean\n",
            "if hasattr(X_holdout, \"columns\"):\n",
            "    feat_names = X_holdout.columns\n",
            "else:\n",
            "    feat_names = [f\"f_{i}\" for i in range(X_holdout.shape[1])]\n",
            "feat_imp_df = pd.DataFrame({'feature': feat_names, 'importance': importances})\n",
            "feat_imp_df = feat_imp_df.sort_values('importance', ascending=False)\n",
            "print(\"Top features by permutation importance on holdout:\")\n",
            "print(feat_imp_df.head(20))\n",
            "\n",
            "# Optional: save results for audit and reproducibility\n",
            "# import joblib\n",
            "# joblib.dump(final_model, 'xgb_final_regularized.pkl')\n",
            "# pd.DataFrame(results).to_csv('random_search_results.csv', index=False)\n",
            "\n",
            "Comments on the code and usage\n",
            "- The code samples conservative hyperparameter combinations that emphasize regularization (lower max_depth, lower learning rates, subsample, reg_alpha/reg_lambda). This is directly motivated by the extreme overfitting seen in your profile (train F1 = 1.0).\n",
            "- The RandomizedSearch loop is written explicitly so we can apply early stopping within each fold (scikit-learn's RandomizedSearchCV does not easily support early stopping on tree boosters per fold).\n",
            "- Permutation importance is computed on the holdout to surface features that the model relies on; features with outsized importance should be audited for leakage.\n",
            "- If you find leakage, fix it and re-run the above (you should re-create holdout before any preprocessing).\n",
            "\n",
            "If you want, I can:\n",
            "- Provide a variant of the above that performs nested cross-validation (outer loop to estimate generalization, inner loop for tuning), or\n",
            "- Provide an analogous pipeline for RandomForest with complexity control and out-of-bag evaluation, or\n",
            "- Help craft the stakeholder slide/email text precisely for your audience and preferred level of technical detail.\n",
            "\n",
            "Which of these would you like next?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49ab1e1"
      },
      "source": [
        "# Task\n",
        "Modify the `results` DataFrame initialization in cell `kLbkUM6A4A5U` to include new index entries for 'holdout_recall', 'holdout_precision', 'holdout_tp', 'holdout_tn', 'holdout_fp', and 'holdout_fn'. Then, in cell `LWuNw9AqtfbH`, after the `f1_score` calculation for each of the Decision Tree, Random Forest, and XGBoost models, add code to compute `recall_score`, `precision_score`, and `confusion_matrix` (extracting TP, TN, FP, FN) on the respective test sets (or encoded test set for XGBoost). Finally, update the `results` DataFrame with these new metrics for each model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47b749a4"
      },
      "source": [
        "## Update results DataFrame initialization\n",
        "\n",
        "### Subtask:\n",
        "Modify the `results` DataFrame initialization to include new index entries for 'holdout_recall', 'holdout_precision', 'holdout_tp', 'holdout_tn', 'holdout_fp', and 'holdout_fn'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8ed2eab"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `results` DataFrame initialization to include new index entries. I will update the code in cell `kLbkUM6A4A5U` to include 'holdout_recall', 'holdout_precision', 'holdout_tp', 'holdout_tn', 'holdout_fp', and 'holdout_fn' alongside the existing entries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a373b0d1",
        "outputId": "df137642-a024-4ade-a2ae-014a8bc2fbc5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "results = pd.DataFrame(index=[\n",
        "    'cv_f1_val',\n",
        "    'cv_std_val',\n",
        "    'cv_f1_train',\n",
        "    'cv_std_train',\n",
        "    'holdout_f1',\n",
        "    'holdout_recall',\n",
        "    'holdout_precision',\n",
        "    'holdout_tp',\n",
        "    'holdout_tn',\n",
        "    'holdout_fp',\n",
        "    'holdout_fn',\n",
        "    'best_params'\n",
        "], columns=['score'])\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  score\n",
            "cv_f1_val           NaN\n",
            "cv_std_val          NaN\n",
            "cv_f1_train         NaN\n",
            "cv_std_train        NaN\n",
            "holdout_f1          NaN\n",
            "holdout_recall      NaN\n",
            "holdout_precision   NaN\n",
            "holdout_tp          NaN\n",
            "holdout_tn          NaN\n",
            "holdout_fp          NaN\n",
            "holdout_fn          NaN\n",
            "best_params         NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dd96237"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires calculating and adding recall, precision, true positives, true negatives, false positives, and false negatives to the `results` DataFrame for each model. This involves modifying the code in cell `LWuNw9AqtfbH` to import necessary metrics, calculate them for each model's predictions, and then update the `results` DataFrame accordingly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "96990e76",
        "outputId": "936f09ec-3dea-47c2-efa2-8392d619500f"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------------\n",
        "# Preprocessing\n",
        "# ---------------------------\n",
        "categorical_transformer = OrdinalEncoder(\n",
        "    handle_unknown='use_encoded_value',\n",
        "    unknown_value=-1\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"cat\", categorical_transformer, cat_columns)\n",
        "], remainder='passthrough')\n",
        "\n",
        "# ---------------------------\n",
        "# CV setup (stratified for classification)\n",
        "# ---------------------------\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# ============================================================\n",
        "# 1) Decision Tree Classifier\n",
        "# ============================================================\n",
        "pipe_dt = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid_dt = {\n",
        "    \"classifier__max_depth\": [1, 5, 10, None],\n",
        "    \"classifier__min_samples_split\": [2, 5, 10],\n",
        "    \"classifier__criterion\": [\"gini\", \"entropy\"]\n",
        "}\n",
        "\n",
        "grid_dt = GridSearchCV(\n",
        "    estimator=pipe_dt,\n",
        "    param_grid=param_grid_dt,\n",
        "    cv=cv,\n",
        "    scoring=\"f1_macro\",\n",
        "    n_jobs=-1,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "grid_dt.fit(X_train, y_train)\n",
        "best_dt = grid_dt.best_estimator_\n",
        "\n",
        "cv_out_dt = cross_validate(\n",
        "    best_dt, X_train, y_train,\n",
        "    cv=cv,\n",
        "    scoring=\"f1_macro\",\n",
        "    return_train_score=True,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "y_pred_dt = best_dt.predict(X_test)\n",
        "holdout_f1_dt = f1_score(y_test, y_pred_dt, pos_label=\"Premium\")\n",
        "holdout_recall_dt = recall_score(y_test, y_pred_dt, pos_label=\"Premium\")\n",
        "holdout_precision_dt = precision_score(y_test, y_pred_dt, pos_label=\"Premium\")\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt, labels=[\"Standard\", \"Premium\"])\n",
        "tn_dt, fp_dt, fn_dt, tp_dt = cm_dt.ravel()\n",
        "\n",
        "results[\"Baseline_DT_F1\"] = [\n",
        "    float(cv_out_dt[\"test_score\"].mean()),\n",
        "    float(cv_out_dt[\"test_score\"].std()),\n",
        "    float(cv_out_dt[\"train_score\"].mean()),\n",
        "    float(cv_out_dt[\"train_score\"].std()),\n",
        "    holdout_f1_dt,\n",
        "    holdout_recall_dt,\n",
        "    holdout_precision_dt,\n",
        "    int(tp_dt),\n",
        "    int(tn_dt),\n",
        "    int(fp_dt),\n",
        "    int(fn_dt),\n",
        "    grid_dt.best_params_\n",
        "]\n",
        "\n",
        "print(\"Decision Tree best params:\", grid_dt.best_params_)\n",
        "\n",
        "# ============================================================\n",
        "# 2) Random Forest Classifier\n",
        "# ============================================================\n",
        "pipe_rf = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid_rf = {\n",
        "    \"classifier__n_estimators\": [100, 200],\n",
        "    \"classifier__max_depth\": [5, 10, None],\n",
        "    \"classifier__min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_rf = GridSearchCV(\n",
        "    estimator=pipe_rf,\n",
        "    param_grid=param_grid_rf,\n",
        "    cv=cv,\n",
        "    scoring=\"f1_macro\",\n",
        "    n_jobs=-1,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "grid_rf.fit(X_train, y_train)\n",
        "best_rf = grid_rf.best_estimator_\n",
        "\n",
        "cv_out_rf = cross_validate(\n",
        "    best_rf, X_train, y_train,\n",
        "    cv=cv,\n",
        "    scoring=\"f1_macro\",\n",
        "    return_train_score=True,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "holdout_f1_rf = f1_score(y_test, y_pred_rf, pos_label=\"Premium\")\n",
        "holdout_recall_rf = recall_score(y_test, y_pred_rf, pos_label=\"Premium\")\n",
        "holdout_precision_rf = precision_score(y_test, y_pred_rf, pos_label=\"Premium\")\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf, labels=[\"Standard\", \"Premium\"])\n",
        "tn_rf, fp_rf, fn_rf, tp_rf = cm_rf.ravel()\n",
        "\n",
        "results[\"Baseline_RF_F1\"] = [\n",
        "    float(cv_out_rf[\"test_score\"].mean()),\n",
        "    float(cv_out_rf[\"test_score\"].std()),\n",
        "    float(cv_out_rf[\"train_score\"].mean()),\n",
        "    float(cv_out_rf[\"train_score\"].std()),\n",
        "    holdout_f1_rf,\n",
        "    holdout_recall_rf,\n",
        "    holdout_precision_rf,\n",
        "    int(tp_rf),\n",
        "    int(tn_rf),\n",
        "    int(fp_rf),\n",
        "    int(fn_rf),\n",
        "    grid_rf.best_params_\n",
        "]\n",
        "\n",
        "print(\"Random Forest best params:\", grid_rf.best_params_)\n",
        "\n",
        "# ============================================================\n",
        "# 3) XGBoost Classifier\n",
        "# ============================================================\n",
        "\n",
        "# Encode target variable for XGBoost\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Determine the numerical label for 'Premium'\n",
        "premium_label = le.transform(['Premium'])[0]\n",
        "standard_label = le.transform(['Standard'])[0]\n",
        "\n",
        "pipe_xgb = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", XGBClassifier(\n",
        "        use_label_encoder=False,\n",
        "        eval_metric=\"logloss\",\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "param_grid_xgb = {\n",
        "    \"classifier__n_estimators\": [100, 200],\n",
        "    \"classifier__max_depth\": [3, 5, 10],\n",
        "    \"classifier__learning_rate\": [0.01, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "grid_xgb = GridSearchCV(\n",
        "    estimator=pipe_xgb,\n",
        "    param_grid=param_grid_xgb,\n",
        "    cv=cv,\n",
        "    scoring=\"f1_macro\",\n",
        "    n_jobs=-1,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "grid_xgb.fit(X_train, y_train_encoded) # Use encoded y_train\n",
        "best_xgb = grid_xgb.best_estimator_\n",
        "\n",
        "cv_out_xgb = cross_validate(\n",
        "    best_xgb, X_train, y_train_encoded, # Use encoded y_train\n",
        "    cv=cv,\n",
        "    scoring=\"f1_macro\",\n",
        "    return_train_score=True,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "y_pred_xgb = best_xgb.predict(X_test)\n",
        "holdout_f1_xgb = f1_score(y_test_encoded, y_pred_xgb, pos_label=premium_label)\n",
        "holdout_recall_xgb = recall_score(y_test_encoded, y_pred_xgb, pos_label=premium_label)\n",
        "holdout_precision_xgb = precision_score(y_test_encoded, y_pred_xgb, pos_label=premium_label)\n",
        "cm_xgb = confusion_matrix(y_test_encoded, y_pred_xgb, labels=[standard_label, premium_label])\n",
        "tn_xgb, fp_xgb, fn_xgb, tp_xgb = cm_xgb.ravel()\n",
        "\n",
        "results[\"Baseline_XGB_F1\"] = [\n",
        "    float(cv_out_xgb[\"test_score\"].mean()),\n",
        "    float(cv_out_xgb[\"test_score\"].std()),\n",
        "    float(cv_out_xgb[\"train_score\"].mean()),\n",
        "    float(cv_out_xgb[\"train_score\"].std()),\n",
        "    holdout_f1_xgb,\n",
        "    holdout_recall_xgb,\n",
        "    holdout_precision_xgb,\n",
        "    int(tp_xgb),\n",
        "    int(tn_xgb),\n",
        "    int(fp_xgb),\n",
        "    int(fn_xgb),\n",
        "    grid_xgb.best_params_\n",
        "]\n",
        "\n",
        "print(\"XGBoost best params:\", grid_xgb.best_params_)\n",
        "\n",
        "# ============================================================\n",
        "# Display combined results\n",
        "# ============================================================\n",
        "display(results)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree best params: {'classifier__criterion': 'entropy', 'classifier__max_depth': None, 'classifier__min_samples_split': 2}\n",
            "Random Forest best params: {'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 200}\n",
            "XGBoost best params: {'classifier__learning_rate': 0.2, 'classifier__max_depth': 10, 'classifier__n_estimators': 200}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                  score                                     Baseline_DT_F1  \\\n",
              "cv_f1_val           NaN                                           0.743689   \n",
              "cv_std_val          NaN                                           0.014803   \n",
              "cv_f1_train         NaN                                                1.0   \n",
              "cv_std_train        NaN                                                0.0   \n",
              "holdout_f1          NaN                                           0.564299   \n",
              "holdout_recall      NaN                                           0.574219   \n",
              "holdout_precision   NaN                                           0.554717   \n",
              "holdout_tp          NaN                                                147   \n",
              "holdout_tn          NaN                                                926   \n",
              "holdout_fp          NaN                                                118   \n",
              "holdout_fn          NaN                                                109   \n",
              "best_params         NaN  {'classifier__criterion': 'entropy', 'classifi...   \n",
              "\n",
              "                                                      Baseline_RF_F1  \\\n",
              "cv_f1_val                                                   0.793656   \n",
              "cv_std_val                                                  0.017083   \n",
              "cv_f1_train                                                      1.0   \n",
              "cv_std_train                                                     0.0   \n",
              "holdout_f1                                                  0.656109   \n",
              "holdout_recall                                              0.566406   \n",
              "holdout_precision                                            0.77957   \n",
              "holdout_tp                                                       145   \n",
              "holdout_tn                                                      1003   \n",
              "holdout_fp                                                        41   \n",
              "holdout_fn                                                       111   \n",
              "best_params        {'classifier__max_depth': None, 'classifier__m...   \n",
              "\n",
              "                                                     Baseline_XGB_F1  \n",
              "cv_f1_val                                                   0.800583  \n",
              "cv_std_val                                                   0.00959  \n",
              "cv_f1_train                                                      1.0  \n",
              "cv_std_train                                                     0.0  \n",
              "holdout_f1                                                  0.653595  \n",
              "holdout_recall                                              0.585938  \n",
              "holdout_precision                                           0.738916  \n",
              "holdout_tp                                                       150  \n",
              "holdout_tn                                                       991  \n",
              "holdout_fp                                                        53  \n",
              "holdout_fn                                                       106  \n",
              "best_params        {'classifier__learning_rate': 0.2, 'classifier...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f7deaf7-97b3-46ad-8c58-7dd3925d46d3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "      <th>Baseline_DT_F1</th>\n",
              "      <th>Baseline_RF_F1</th>\n",
              "      <th>Baseline_XGB_F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>cv_f1_val</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.743689</td>\n",
              "      <td>0.793656</td>\n",
              "      <td>0.800583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cv_std_val</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.014803</td>\n",
              "      <td>0.017083</td>\n",
              "      <td>0.00959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cv_f1_train</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cv_std_train</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>holdout_f1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.564299</td>\n",
              "      <td>0.656109</td>\n",
              "      <td>0.653595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>holdout_recall</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.574219</td>\n",
              "      <td>0.566406</td>\n",
              "      <td>0.585938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>holdout_precision</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.554717</td>\n",
              "      <td>0.77957</td>\n",
              "      <td>0.738916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>holdout_tp</th>\n",
              "      <td>NaN</td>\n",
              "      <td>147</td>\n",
              "      <td>145</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>holdout_tn</th>\n",
              "      <td>NaN</td>\n",
              "      <td>926</td>\n",
              "      <td>1003</td>\n",
              "      <td>991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>holdout_fp</th>\n",
              "      <td>NaN</td>\n",
              "      <td>118</td>\n",
              "      <td>41</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>holdout_fn</th>\n",
              "      <td>NaN</td>\n",
              "      <td>109</td>\n",
              "      <td>111</td>\n",
              "      <td>106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>best_params</th>\n",
              "      <td>NaN</td>\n",
              "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
              "      <td>{'classifier__max_depth': None, 'classifier__m...</td>\n",
              "      <td>{'classifier__learning_rate': 0.2, 'classifier...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f7deaf7-97b3-46ad-8c58-7dd3925d46d3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2f7deaf7-97b3-46ad-8c58-7dd3925d46d3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2f7deaf7-97b3-46ad-8c58-7dd3925d46d3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_afb4c572-ebbf-440a-ad76-d42483df7ab3\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_afb4c572-ebbf-440a-ad76-d42483df7ab3 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "195bc990"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The task was to modify the `results` DataFrame initialization to include new index entries for 'holdout\\_recall', 'holdout\\_precision', 'holdout\\_tp', 'holdout\\_tn', 'holdout\\_fp', and 'holdout\\_fn', and then to calculate and update these metrics for Decision Tree, Random Forest, and XGBoost models. This task was successfully completed.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `results` DataFrame was successfully initialized with new index entries: 'holdout\\_recall', 'holdout\\_precision', 'holdout\\_tp', 'holdout\\_tn', 'holdout\\_fp', and 'holdout\\_fn'.\n",
        "*   For each of the three models (Decision Tree, Random Forest, and XGBoost), `recall_score`, `precision_score`, and `confusion_matrix` components (true positives, true negatives, false positives, and false negatives) were computed on their respective holdout test sets.\n",
        "*   These newly calculated metrics were successfully integrated into the `results` DataFrame for each model, alongside the existing cross-validation F1-scores and best parameters.\n",
        "*   The process required including previous data loading, preprocessing, and train-test split steps within the execution context to resolve dependency errors like undefined `X_train` or `cat_columns`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The expanded `results` DataFrame now provides a more comprehensive overview of model performance beyond just the F1-score, allowing for a deeper understanding of classification behavior, especially regarding False Positives and False Negatives, which can be critical depending on the business problem.\n",
        "*   Further analysis can be conducted to compare the models' trade-offs between precision and recall, informing decisions on which model is most suitable based on the specific costs associated with different types of errors (e.g., false positives vs. false negatives).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Model Performance Analysis ###\")\n",
        "print(\"\\n--- Overall Observations ---\")\n",
        "print(\"1.  **Severe Overfitting**: All models (Decision Tree, Random Forest, XGBoost) show perfect F1-scores (1.0) on the training data (`cv_f1_train`) but significantly lower F1-scores on both the cross-validation set (`cv_f1_val`) and the final holdout test set (`holdout_f1`). This indicates that the models are memorizing the training data and failing to generalize well to unseen data.\")\n",
        "print(\"    - Decision Tree CV Train F1: 1.0 vs CV Val F1: 0.744 vs Holdout F1: 0.564\")\n",
        "print(\"    - Random Forest CV Train F1: 1.0 vs CV Val F1: 0.794 vs Holdout F1: 0.656\")\n",
        "print(\"    - XGBoost CV Train F1: 1.0 vs CV Val F1: 0.801 vs Holdout F1: 0.654\")\n",
        "print(\"    This gap suggests the models are too complex for the given data or that hyperparameters are not sufficiently regularizing the models. The `max_depth=None` for DT and RF from the best parameters reinforces this.\")\n",
        "print(\"2.  **Discrepancy between Cross-Validation and Holdout**: There's a noticeable drop in F1-score from the cross-validation set to the final holdout set for Random Forest and XGBoost (approx. 0.14-0.15 points). This suggests potential issues like:\")\n",
        "print(\"    - Hyperparameters tuned on CV folds are not generalizing to the truly unseen holdout set.\")\n",
        "print(\"    - Possible subtle data leakage within the CV process or a distribution shift between the CV splits and the holdout set.\")\n",
        "\n",
        "print(\"\\n--- Model Comparison on Holdout Set ---\")\n",
        "print(\"We'll compare models primarily based on their performance on the holdout set, as this represents truly unseen data.\")\n",
        "\n",
        "# Assuming these variables hold the correct values from the latest execution\n",
        "# Decision Tree Metrics\n",
        "decision_tree_f1 = 0.564299\n",
        "decision_tree_recall = 0.57421875\n",
        "decision_tree_precision = 0.55471698\n",
        "decision_tree_tp = 147\n",
        "decision_tree_tn = 926\n",
        "decision_tree_fp = 118\n",
        "decision_tree_fn = 109\n",
        "\n",
        "# Random Forest Metrics\n",
        "random_forest_f1 = 0.6561085972850679\n",
        "random_forest_recall = 0.56640625\n",
        "random_forest_precision = 0.7795698924731183\n",
        "random_forest_tp = 145\n",
        "random_forest_tn = 1003\n",
        "random_forest_fp = 41\n",
        "random_forest_fn = 111\n",
        "\n",
        "# XGBoost Metrics\n",
        "xgboost_f1 = 0.6535947712418301\n",
        "xgboost_recall = 0.5859375\n",
        "xgboost_precision = 0.7389162561576355\n",
        "xgboost_tp = 150\n",
        "xgboost_tn = 991\n",
        "xgboost_fp = 53\n",
        "xgboost_fn = 106\n",
        "\n",
        "print(\"\\n**Decision Tree Classifier**:\")\n",
        "print(f\"- Holdout F1-score: {decision_tree_f1:.3f}\")\n",
        "print(f\"- Holdout Recall: {decision_tree_recall:.3f} (correctly identified {decision_tree_tp} out of {decision_tree_tp + decision_tree_fn} Premium wines)\")\n",
        "print(f\"- Holdout Precision: {decision_tree_precision:.3f} ({decision_tree_tp} of {decision_tree_tp + decision_tree_fp} predicted Premium wines were actually Premium)\")\n",
        "print(f\"- True Positives (TP): {decision_tree_tp}, True Negatives (TN): {decision_tree_tn}, False Positives (FP): {decision_tree_fp}, False Negatives (FN): {decision_tree_fn}\")\n",
        "\n",
        "print(\"\\n**Random Forest Classifier**:\")\n",
        "print(f\"- Holdout F1-score: {random_forest_f1:.3f}\")\n",
        "print(f\"- Holdout Recall: {random_forest_recall:.3f} (correctly identified {random_forest_tp} out of {random_forest_tp + random_forest_fn} Premium wines)\")\n",
        "print(f\"- Holdout Precision: {random_forest_precision:.3f} ({random_forest_tp} of {random_forest_tp + random_forest_fp} predicted Premium wines were actually Premium)\")\n",
        "print(f\"- True Positives (TP): {random_forest_tp}, True Negatives (TN): {random_forest_tn}, False Positives (FP): {random_forest_fp}, False Negatives (FN): {random_forest_fn}\")\n",
        "\n",
        "print(\"\\n**XGBoost Classifier**:\")\n",
        "print(f\"- Holdout F1-score: {xgboost_f1:.3f}\")\n",
        "print(f\"- Holdout Recall: {xgboost_recall:.3f} (correctly identified {xgboost_tp} out of {xgboost_tp + xgboost_fn} Premium wines)\")\n",
        "print(f\"- Holdout Precision: {xgboost_precision:.3f} ({xgboost_tp} of {xgboost_tp + xgboost_fp} predicted Premium wines were actually Premium)\")\n",
        "print(f\"- True Positives (TP): {xgboost_tp}, True Negatives (TN): {xgboost_tn}, False Positives (FP): {xgboost_fp}, False Negatives (FN): {xgboost_fn}\")\n",
        "\n",
        "print(\"\\n--- Which model performed the best? ---\")\n",
        "print(\"Comparing the holdout F1-scores, **Random Forest (0.656) and XGBoost (0.654) perform very similarly and are significantly better than the single Decision Tree (0.564)**.\")\n",
        "print(\"\\nTo differentiate between Random Forest and XGBoost:\")\n",
        "print(\"-   **Random Forest** achieves a slightly higher F1-score and notably higher **Precision (0.780)**. This means when Random Forest predicts a wine is 'Premium', it's correct about 78% of the time, resulting in fewer false alarms (only 41 False Positives). This is crucial if misclassifying a 'Standard' wine as 'Premium' is costly for the business.\")\n",
        "print(\"-   **XGBoost** has a slightly higher **Recall (0.586)**, meaning it's slightly better at identifying a larger proportion of the actual 'Premium' wines, resulting in more True Positives (150 compared to RF's 145). This might be preferred if missing a 'Premium' wine is more costly.\")\n",
        "print(\"\\n**Conclusion**: If the business prioritizes minimizing false positives (i.e., not incorrectly marketing a standard wine as premium), **Random Forest is marginally the best performing model** due to its higher precision. If identifying as many premium wines as possible (even with some false positives) is the priority, XGBoost could be slightly preferred due to its higher recall. However, both ensemble models clearly outperform the single Decision Tree.\")\n",
        "\n",
        "print(\"\\n--- Next Steps ---\")\n",
        "print(\"Given the severe overfitting, the next crucial steps should focus on regularization and potentially re-evaluating the preprocessing or feature engineering to improve generalization, as highlighted in the previous LLM response.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LakveX3d8bSe",
        "outputId": "99e66916-e90e-4b8b-9aa4-27a083380add"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Model Performance Analysis ###\n",
            "\n",
            "--- Overall Observations ---\n",
            "1.  **Severe Overfitting**: All models (Decision Tree, Random Forest, XGBoost) show perfect F1-scores (1.0) on the training data (`cv_f1_train`) but significantly lower F1-scores on both the cross-validation set (`cv_f1_val`) and the final holdout test set (`holdout_f1`). This indicates that the models are memorizing the training data and failing to generalize well to unseen data.\n",
            "    - Decision Tree CV Train F1: 1.0 vs CV Val F1: 0.744 vs Holdout F1: 0.564\n",
            "    - Random Forest CV Train F1: 1.0 vs CV Val F1: 0.794 vs Holdout F1: 0.656\n",
            "    - XGBoost CV Train F1: 1.0 vs CV Val F1: 0.801 vs Holdout F1: 0.654\n",
            "    This gap suggests the models are too complex for the given data or that hyperparameters are not sufficiently regularizing the models. The `max_depth=None` for DT and RF from the best parameters reinforces this.\n",
            "2.  **Discrepancy between Cross-Validation and Holdout**: There's a noticeable drop in F1-score from the cross-validation set to the final holdout set for Random Forest and XGBoost (approx. 0.14-0.15 points). This suggests potential issues like:\n",
            "    - Hyperparameters tuned on CV folds are not generalizing to the truly unseen holdout set.\n",
            "    - Possible subtle data leakage within the CV process or a distribution shift between the CV splits and the holdout set.\n",
            "\n",
            "--- Model Comparison on Holdout Set ---\n",
            "We'll compare models primarily based on their performance on the holdout set, as this represents truly unseen data.\n",
            "\n",
            "**Decision Tree Classifier**:\n",
            "- Holdout F1-score: 0.564\n",
            "- Holdout Recall: 0.574 (correctly identified 147 out of 256 Premium wines)\n",
            "- Holdout Precision: 0.555 (147 of 265 predicted Premium wines were actually Premium)\n",
            "- True Positives (TP): 147, True Negatives (TN): 926, False Positives (FP): 118, False Negatives (FN): 109\n",
            "\n",
            "**Random Forest Classifier**:\n",
            "- Holdout F1-score: 0.656\n",
            "- Holdout Recall: 0.566 (correctly identified 145 out of 256 Premium wines)\n",
            "- Holdout Precision: 0.780 (145 of 186 predicted Premium wines were actually Premium)\n",
            "- True Positives (TP): 145, True Negatives (TN): 1003, False Positives (FP): 41, False Negatives (FN): 111\n",
            "\n",
            "**XGBoost Classifier**:\n",
            "- Holdout F1-score: 0.654\n",
            "- Holdout Recall: 0.586 (correctly identified 150 out of 256 Premium wines)\n",
            "- Holdout Precision: 0.739 (150 of 203 predicted Premium wines were actually Premium)\n",
            "- True Positives (TP): 150, True Negatives (TN): 991, False Positives (FP): 53, False Negatives (FN): 106\n",
            "\n",
            "--- Which model performed the best? ---\n",
            "Comparing the holdout F1-scores, **Random Forest (0.656) and XGBoost (0.654) perform very similarly and are significantly better than the single Decision Tree (0.564)**.\n",
            "\n",
            "To differentiate between Random Forest and XGBoost:\n",
            "-   **Random Forest** achieves a slightly higher F1-score and notably higher **Precision (0.780)**. This means when Random Forest predicts a wine is 'Premium', it's correct about 78% of the time, resulting in fewer false alarms (only 41 False Positives). This is crucial if misclassifying a 'Standard' wine as 'Premium' is costly for the business.\n",
            "-   **XGBoost** has a slightly higher **Recall (0.586)**, meaning it's slightly better at identifying a larger proportion of the actual 'Premium' wines, resulting in more True Positives (150 compared to RF's 145). This might be preferred if missing a 'Premium' wine is more costly.\n",
            "\n",
            "**Conclusion**: If the business prioritizes minimizing false positives (i.e., not incorrectly marketing a standard wine as premium), **Random Forest is marginally the best performing model** due to its higher precision. If identifying as many premium wines as possible (even with some false positives) is the priority, XGBoost could be slightly preferred due to its higher recall. However, both ensemble models clearly outperform the single Decision Tree.\n",
            "\n",
            "--- Next Steps ---\n",
            "Given the severe overfitting, the next crucial steps should focus on regularization and potentially re-evaluating the preprocessing or feature engineering to improve generalization, as highlighted in the previous LLM response.\n"
          ]
        }
      ]
    }
  ]
}